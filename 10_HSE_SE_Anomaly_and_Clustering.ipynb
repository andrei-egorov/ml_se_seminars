{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3NrRk6skjKq"
   },
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Seminar 10: Anomaly Detection & Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_gITGEgkjKv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNvaQyPTkjKx"
   },
   "source": [
    "# Outline\n",
    "\n",
    "## Anomaly Detection\n",
    "- [One-class SVM](#one-class_svm)\n",
    "- [Isolation Forest](#isolation_forest)\n",
    "- [CUSUM](#cumsum)\n",
    "\n",
    "## Clustering\n",
    "- [k-means](#k-means)\n",
    "- [DBSCAN](#dbscan)\n",
    "- [Hierarchical clustering](#Hierarchical-clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gflmgy6DkjKx"
   },
   "source": [
    "## Data preparation & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXK65cWokjKy"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('winequality-white.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "58NxB3MBkjKy",
    "outputId": "16299458-08b0-4e8a-8042-82619b2f2228"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "PWazB63YkjKz",
    "outputId": "29fbb139-63b2-4363-c7cf-0baa8a498ee8"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vv_0QnzekjKz",
    "outputId": "329f2df9-3bf3-4662-daf3-e4d6c63f286f"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "41AFO1c9kjK0",
    "outputId": "61831eb1-151f-4724-9b36-63bb73a186e4"
   },
   "outputs": [],
   "source": [
    "data.hist(figsize=(20, 20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "KbKABLfHkjK0",
    "outputId": "0573b3b7-6b31-4e6d-8885-9baeaddf3e6d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "corr_matrix = data.corr()\n",
    "lower = corr_matrix.where(\n",
    "    np.tril(np.ones(corr_matrix.shape), k=-1).astype(np.bool)\n",
    ")\n",
    "\n",
    "sns.heatmap(lower, annot=True, fmt='.2f', cbar=False, center=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9SxXBBRkjK1",
    "outputId": "3008395f-b350-43ff-dba2-98019e0ea9a1"
   },
   "outputs": [],
   "source": [
    "high_corr = [\n",
    "    column for column in lower.columns if any(\n",
    "        (lower[column] > 0.6)|(lower[column] < -0.6)\n",
    "    )\n",
    "]\n",
    "high_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTlSfoy_kjK1"
   },
   "outputs": [],
   "source": [
    "other_features = [col for col in data.columns if col not in high_corr and col!='quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "WxG_fU2QkjK1",
    "outputId": "f25f613f-7f53-4afe-f5b1-221ef0f7ef72"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data[high_corr]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "22a64p4lkjK2",
    "outputId": "21e7b39b-27ed-4c4a-dfc9-8b0660ac91ee"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data[other_features]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5-3XZ5EkjK2"
   },
   "source": [
    "## The Simple method\n",
    "\n",
    "### 3-$\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiVxR-DSkjK2"
   },
   "outputs": [],
   "source": [
    "def outlier_std(data, col, threshold=3):\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    up_bound = mean + threshold * std\n",
    "    low_bound = mean - threshold * std\n",
    "    anomalies = pd.concat([data[col]>up_bound, data[col]<low_bound], axis=1).any(1)\n",
    "    return anomalies, up_bound, low_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdbkpmaukjK3"
   },
   "outputs": [],
   "source": [
    "def get_column_outliers(data, columns=None, function=outlier_std, threshold=3):\n",
    "    if columns:\n",
    "        columns_to_check = columns\n",
    "    else:\n",
    "        columns_to_check = data.columns\n",
    "        \n",
    "    outliers = pd.Series(\n",
    "        data=[False]*len(data), index=data_features.index, name='is_outlier'\n",
    "    )\n",
    "    \n",
    "    comparison_table = {}\n",
    "    for column in columns_to_check:\n",
    "        anomalies, upper_bound, lower_bound = function(\n",
    "            data, column, threshold=threshold\n",
    "        )\n",
    "        \n",
    "        comparison_table[column] = \\\n",
    "        [upper_bound, lower_bound, sum(anomalies), 100*sum(anomalies)/len(anomalies)]\n",
    "        outliers[anomalies[anomalies].index] = True\n",
    "    \n",
    "    comparison_table = pd.DataFrame(comparison_table).T\n",
    "    comparison_table.columns=[\n",
    "        'upper_bound', 'lower_bound', 'anomalies_count', 'anomalies_percentage'\n",
    "    ]\n",
    "    \n",
    "    comparison_table = comparison_table.sort_values(\n",
    "        by='anomalies_percentage', ascending=False\n",
    "    )\n",
    "    \n",
    "    return comparison_table, outliers\n",
    "\n",
    "def anomalies_report(outliers):\n",
    "    print(\"Total number of outliers: {}\\nPercentage of outliers:   {:.2f}%\".format(\n",
    "            sum(outliers), 100*sum(outliers)/len(outliers)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLVDE1KakjK3"
   },
   "outputs": [],
   "source": [
    "data_features = data.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6NsEIofkjK4",
    "outputId": "39eae936-2f17-4a3f-c353-94ce98cfafd3"
   },
   "outputs": [],
   "source": [
    "comparison_table, std_outliers = get_column_outliers(data_features)\n",
    "anomalies_report(std_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "WasVbPaikjK4",
    "outputId": "df6faaa6-bcd0-428d-fb45-7c513a689e3a"
   },
   "outputs": [],
   "source": [
    "comparison_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yaFudMP9kjK5",
    "outputId": "f09049f7-82cf-48e0-b2da-ba3527a265a9"
   },
   "outputs": [],
   "source": [
    "# ~50 sec\n",
    "labeled_data = data_features.copy()\n",
    "labeled_data['is_outlier'] = std_outliers\n",
    "\n",
    "sns.pairplot(data=labeled_data, vars = other_features,\n",
    "             hue='is_outlier', hue_order=[1, 0],\n",
    "             markers=['X', 'o'],  palette='bright');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvEF-2TrkjK5"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaled_data = pd.DataFrame(\n",
    "    data=scaler.fit_transform(data_features), \n",
    "    columns=data_features.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sjqdHqykjK5"
   },
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "\n",
    "`Scikit-learn` has several anomalies detection algorithms. Their description and examples are provided on [this page](https://scikit-learn.org/stable/modules/outlier_detection.html):\n",
    "\n",
    "<center><img src=\"anomaly.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX-A_GFokjK5"
   },
   "source": [
    "## One-class SVM <a id=\"one-class_svm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itV_12ngkjK6"
   },
   "source": [
    "## Brielfy about Support Vector Machine\n",
    "\n",
    "In short, the SVM is a basic linear model. The main idea of the algorithm (in the case of classification) is to divide the classes by a hyperplane so as to maximize the distance (gap) between them. Initially, the algorithm was able to work only with linearly separable classes, but in the 90s of the last century, the method became especially popular due to the introduction of the \"Kernel Trick\" (1992), which made it possible to work effectively with linearly inseparable data.\n",
    "\n",
    "![](https://sandipanweb.files.wordpress.com/2018/04/svm_slack.png?w=676)\n",
    "\n",
    "## Kernel Trick\n",
    "\n",
    "The kernel is a function that can transform the feature space (including non-linearly), without directly transforming the features.\n",
    "\n",
    "It is extremely efficient in terms of calculation and potentially allows you to obtain infinite-dimensional feature spaces.\n",
    "\n",
    "The idea is that classes that are linearly inseparable in the current feature space can become separable in higher-dimensional spaces:\n",
    "\n",
    "![](https://miro.medium.com/max/1838/1*eU9PzjVcLNbNEzBC2g_iWg.jpeg)\n",
    "\n",
    "## Algorithm itself (Scholkopf approach)\n",
    "http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/\n",
    "\n",
    "**One Class SVM** - this is one of the forms of the classical algorithm, but, as the name implies, to train it, we only need to have one class, even if it is a little \"noisy\", and we want to learn how to make a decision for each new observation, whether it is anomalous or not.\n",
    "\n",
    "**General Idea:** transform the feature space and draw the separating hyperplane so that the observations lie as far away from the origin as possible:\n",
    "\n",
    "![](https://www.researchgate.net/profile/Hany_Alashwal/publication/242572058/figure/fig1/AS:393295319584771@1470780319210/Classification-in-one-class-SVM.png)\n",
    "\n",
    "As a result, we get a boundary on one side of which lie the most \"dense\" and similar observations from our training sample, and on the other side there will be anomalous values that are not similar to all the others. The percentage of such anomalous observations that the model will try to separate from the main part of the sample, we again set at the very beginning of training using the parameter `nu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hubWaN3IkjK6"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "one_class_svm = OneClassSVM(nu=0.10, gamma='auto')\n",
    "one_class_svm.fit(scaled_data)\n",
    "svm_outliers = one_class_svm.predict(scaled_data)\n",
    "svm_outliers = np.array([1 if label == -1 else 0 for label in svm_outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Zqfv_qpkjK6",
    "outputId": "aac232d5-08b4-41b5-f787-d64f0d90d5e1"
   },
   "outputs": [],
   "source": [
    "anomalies_report(svm_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ivoEenGqkjK6",
    "outputId": "27c16747-f673-4625-ce82-8eaf6f33c534"
   },
   "outputs": [],
   "source": [
    "# ~51 sec\n",
    "labeled_data = data_features.copy()\n",
    "labeled_data['is_outlier'] = svm_outliers\n",
    "\n",
    "sns.pairplot(data=labeled_data, vars = other_features,\n",
    "             hue='is_outlier', hue_order=[1, 0],\n",
    "             markers=['X', 'o'],  palette='bright');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVumNE28kjK7"
   },
   "source": [
    "## Pros and cons\n",
    "\n",
    "\\+ Due to kernel trick, the model is able to draw **non-linear dividing boundaries**\n",
    "\n",
    "\\+ It is especially convenient to use when there are not enough \"bad\" observations in the data to use the standard approach of learning with a teacher - binary classification\n",
    "\n",
    "\\- It can be very **over-trained** and produce a large number of false negative results if the separation gap is too small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoaRfevIkjK7"
   },
   "source": [
    "## Isolation Forest <a id=\"isolation_forest\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g7pSe7DkjK7"
   },
   "source": [
    "The idea - let's see how easy it is to \"isolate\" an observation from everyone else. If it is too easy, it probably lies far away and is an outlier. If it is very heavy - most likely it is similar to a bunch of other points and is not an outlier.\n",
    "\n",
    "\n",
    "**Algorithm:**\n",
    "1. Select the point to isolate.\n",
    "2. For each feature, set the isolation range between minimum and maximum.\n",
    "3. Select a feature at random.\n",
    "4. Select a value that is in the range, again randomly:\n",
    "    - If the point is higher than the selected value, move the minimum range of the feature by this value.\n",
    "    - If the point is lower, move the maximum value of the feature by the value.\n",
    "5. Repeat steps 3 and 4 until the point is isolated. That is, until the point becomes the only one located within the range for all the features.\n",
    "6. Count the number of times you had to repeat steps 3 and 4.\n",
    "\n",
    "https://quantdare.com/isolation-forest-algorithm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJTIwXdLkjK8"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isolation_forest = IsolationForest(n_estimators=100, contamination=0.1, \n",
    "                                   max_features=1.0, bootstrap=True)\n",
    "isolation_forest.fit(scaled_data)\n",
    "\n",
    "isolation_outliers = isolation_forest.predict(scaled_data)\n",
    "isolation_outliers = np.array([1 if label == -1 else 0 for label in isolation_outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jw__bzlbkjK8",
    "outputId": "dc03715c-a87e-4b41-fd69-a7c402905c46"
   },
   "outputs": [],
   "source": [
    "anomalies_report(isolation_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MsUiyLFXkjK8",
    "outputId": "558fb280-ea82-4f3c-bcda-ebf58f4b9cdb"
   },
   "outputs": [],
   "source": [
    "# ~50 sec\n",
    "labeled_data = data_features.copy()\n",
    "labeled_data['is_outlier'] = isolation_outliers\n",
    "\n",
    "sns.pairplot(data=labeled_data, vars = other_features,\n",
    "             hue='is_outlier', hue_order=[1, 0],\n",
    "             markers=['X', 'o'],  palette='bright');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ7hv1P0kjK9"
   },
   "source": [
    "## Pros and cons\n",
    "\n",
    "\\+ Again **non-linear separating boundaries**, intuitive working principles\n",
    "\n",
    "\\+ Again, it is convenient to use when we can not enter the binary classification\n",
    "\n",
    "\\- A rather complex parameter setting, which is difficult to carry out without at least a minimal markup or an assumption about the number of \"dirty\" observations-the `contamination`parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNuVdkNYkjK9"
   },
   "source": [
    "## CUSUM <a id=\"cumsum\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMWp_WCxkjK9"
   },
   "source": [
    "[Change detection](http://en.wikipedia.org/wiki/Change_detection) refers to procedures to identify abrupt changes in a phenomenon (Basseville and Nikiforov 1993, Gustafsson 2000). By abrupt change it is meant any difference in relation to previous known data faster than expected of some characteristic of the data such as amplitude, mean, variance, frequency, etc.\n",
    "\n",
    "The [Cumulative sum (CUSUM)](http://en.wikipedia.org/wiki/CUSUM) algorithm is a classical technique for monitoring change detection. One form of implementing the CUSUM algorithm involves the calculation of the cumulative sum of positive and negative changes ($g_t^+$ and $g_t^-$) in the data ($x$) and comparison to a $threshold$. When this threshold is exceeded a change is detected ($t_{talarm}$) and the cumulative sum restarts from zero. To avoid the detection of a change in absence of an actual change or a slow drift, this algorithm also depends on a parameter $drift$ for drift correction. This form of the CUSUM algorithm is given by:\n",
    "\n",
    "$$ \\begin{array}{l l} \n",
    "\\left\\{ \\begin{array}{l l} \n",
    "s[t] = x[t] - x[t-1] \\\\\n",
    "g^+[t] = max\\left(g^+[t-1] + s[t]-drift,\\; 0\\right) \\\\\n",
    "g^-[t] = max\\left(g^-[t-1] - s[t]-drift,\\; 0\\right)\n",
    "\\end{array} \\right. \\\\\n",
    "\\\\\n",
    "\\; if \\;\\;\\; g^+[t] > threshold \\;\\;\\; or \\;\\;\\;  g^-[t] > threshold: \\\\\n",
    "\\\\\n",
    "\\left\\{ \\begin{array}{l l} \n",
    "t_{talarm}=t \\\\\n",
    "g^+[t] = 0 \\\\\n",
    "g^-[t] = 0 \n",
    "\\end{array} \\right.\n",
    "\\end{array} $$\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "There are different implementations of the CUSUM algorithm; for example, the term for the sum of the last elements ($s[t]$ above) can have a longer history (with filtering), it can be normalized by removing the data mean and then divided by the data variance), or this sum term can be squared for detecting both variance and parameter changes, etc.\n",
    "\n",
    "For the CUSUM algorithm to work properly, it depends on tuning the parameters $h$ and $v$ to what is meant by a change in the data. According to Gustafsson (2000), this tuning can be performed following these steps:\n",
    "\n",
    " - Start with a very large $threshold$.  \n",
    " - Choose $drift$ to one half of the expected change, or adjust $drift$ such that $g$ = 0 more than 50% of the time.  \n",
    " - Then set the $threshold$ so the required number of false alarms (this can be done automatically) or delay for detection is obtained.  \n",
    " - If faster detection is sought, try to decrease $drift$.  \n",
    " - If fewer false alarms are wanted, try to increase $drift$.   \n",
    " - If there is a subset of the change times that does not make sense, try to increase $drift$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqKhqQNokjK9",
    "outputId": "413bb075-6034-4dcd-a960-67b1f14e1fd6"
   },
   "outputs": [],
   "source": [
    "!pip install detecta --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0QFGZOSkjK-"
   },
   "source": [
    "The function `detect_cusum.py` from Python module `detecta` implements the CUSUM algorithm and a procedure to calculate the ending of the detected change. The function signature is:   \n",
    "\n",
    "```python\n",
    "ta, tai, taf, amp = detect_cusum(x, threshold=1, drift=0, ending=False, show=True, ax=None)\n",
    "```   \n",
    "\n",
    "Let's see how to use `detect_cusum.py`; first let's import the necessary Python libraries and configure the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lwgcLdyhkjK-",
    "outputId": "3a9a10e8-d8dc-4d32-c04e-b73cd1b1c64b"
   },
   "outputs": [],
   "source": [
    "from detecta import detect_cusum\n",
    "\n",
    "x = np.random.randn(300)/5\n",
    "x[100:200] += np.arange(0, 4, 4/100)\n",
    "ta, tai, taf, amp = detect_cusum(x, 2, .02, True, True)\n",
    "\n",
    "x = np.random.randn(300)\n",
    "detect_cusum(x, 4, 1.5, True, True)\n",
    "    \n",
    "x = 2*np.sin(2*np.pi*np.arange(0, 3, .01))\n",
    "ta, tai, taf, amp = detect_cusum(x, 1, .05, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxJXcyzkkjK_"
   },
   "source": [
    "## References\n",
    "\n",
    "- Michèle Basseville and Igor V. Nikiforov (1993). [Detection of Abrupt Changes: Theory and Application](http://books.google.com.br/books/about/Detection_of_abrupt_changes.html?id=Vu5SAAAAMAAJ). Prentice-Hall.   \n",
    "- Fredrik Gustafsson (2000) [Adaptive Filtering and Change Detection](http://books.google.com.br/books?id=cyNTAAAAMAAJ). Wiley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw9AbYvCkjLA"
   },
   "source": [
    "## k-Means <a id=\"k-means\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQy138fRkjLA"
   },
   "source": [
    "K-means clustering is a simple and elegant approach for partitioning a data set into $K$ distinct, non-overlapping clusters. To perform K-means clustering, we must first specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the $K$ clusters.\n",
    "\n",
    "\n",
    "We begin by defining some notation. Let $C_1, ..., C_K$ denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:\n",
    "1. Each observation belongs to at least one of the K clusters.\n",
    "2. The clusters are nonoverlapping: no observation belongs to more than one cluster.\n",
    "\n",
    "\n",
    "The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The within-cluster variation for cluster $C_k$ is a measure $W(C_k)$ of the amount by which the observations\n",
    "within a cluster differ from each other. Hence we want to solve the problem $$\\sum_{k=1}^K W(C_k) \\to \\min_{C_k}.$$ In words, this formula says that we want to partition the observations into $K$ clusters such that the total within-cluster variation, summed over all $K$ clusters, is as small as possible.\n",
    "\n",
    "In order to make it actionable we need to define the within-cluster variation. There are many possible ways to define this concept, but by far the most common choice involves squared Euclidean distance. That is, we define $$W(C_k) = \\frac{1}{|C_k|}\\sum_{i,i'\\in C_k}\\sum_{j=1}^p\\left(x_{ij}-x_{i'j}\\right)^2.$$ In other words, the within-cluster variation for the kth cluster is the sum of all of the pairwise squared Euclidean distances between the observations in the k-th cluster, divided by the total number of observations in the k-th cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgXOmkrYkjLB"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oOXOJK2kjLC"
   },
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "random_state = 170\n",
    "\n",
    "#toy datasets\n",
    "X_blobs, y_blobs = datasets.make_blobs(\n",
    "    centers=3, n_samples=n_samples, random_state=random_state, center_box=(-10, 10)\n",
    ")\n",
    "X_circles, y_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "X_moons, y_moons = datasets.make_moons(n_samples=n_samples, noise=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZstkYlrkjLC",
    "outputId": "286b6df5-a454-43a1-f73f-ce80edaf5059"
   },
   "outputs": [],
   "source": [
    "X_blobs[:5], X_circles[:5], X_moons[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0_0M02zkjLC",
    "outputId": "55a9e669-33c4-44f0-eada-8baee4782403"
   },
   "outputs": [],
   "source": [
    "y_blobs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YWszSLWkjLC"
   },
   "outputs": [],
   "source": [
    "def plot_clusters(X, y):\n",
    "\n",
    "    # Create an figure with a custom size\n",
    "    # plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    if y is not None:\n",
    "        for cluster_label in np.unique(y):\n",
    "            # Plot all objects with y == i (class 0)\n",
    "            plt.scatter(X[y == cluster_label, 0],     \n",
    "                        # selects all objects with y == i and the 1st column of X\n",
    "                        X[y == cluster_label, 1],     \n",
    "                        # selects all objects with y == i and the 2nd column of X\n",
    "                        label=str(cluster_label))     \n",
    "                        # label for the plot legend\n",
    "    else:\n",
    "        plt.scatter(X[:, 0], X[:, 1], label='samples')\n",
    "\n",
    "    plt.xlabel('X1', size=12) # set up X-axis label\n",
    "    plt.ylabel('X2', size=12) # set up Y-axis label\n",
    "    plt.xticks(size=12)\n",
    "    plt.yticks(size=12)\n",
    "\n",
    "    plt.legend(loc='best', fontsize=12) # create the plot legend and set up it position\n",
    "    plt.grid(b=1) # create grid on the plot\n",
    "\n",
    "    plt.show() # display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "eLCLoBWZkjLD",
    "outputId": "bf61a723-908c-4c32-b819-108830dea57f"
   },
   "outputs": [],
   "source": [
    "plot_clusters(X_blobs, y=None)\n",
    "plot_clusters(X_circles, y=None)\n",
    "plot_clusters(X_moons, y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4Nn-klXkjLD"
   },
   "source": [
    "### K-Means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlYJT4DekjLD"
   },
   "outputs": [],
   "source": [
    "#sklearn kmeans implementation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, max_iter=20, n_init=10)\n",
    "kmeans.fit(X_blobs)\n",
    "y_pred_blobs = kmeans.predict(X_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "MGAxrQQekjLD",
    "outputId": "a2d6258f-f959-483d-ea49-fc9280dd18ef"
   },
   "outputs": [],
   "source": [
    "plot_clusters(X_blobs, y_pred_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esJ3dreFkjLD"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, max_iter=20, n_init=10)\n",
    "kmeans.fit(X_circles)\n",
    "y_pred_circles = kmeans.predict(X_circles)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, max_iter=20, n_init=10)\n",
    "kmeans.fit(X_moons)\n",
    "y_pred_moons = kmeans.predict(X_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "8UMS3amzkjLE",
    "outputId": "588a9095-34ec-46cd-ead9-ef0b6ea9865f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_clusters(X_circles, y_pred_circles)\n",
    "plot_clusters(X_moons, y_pred_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uv6VeJzckjLE"
   },
   "source": [
    "K-means doesn't work well here in this example. We need to use some other clustering algorithm here. Which one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcTdNMhekjLE"
   },
   "source": [
    "## Metrics\n",
    "\n",
    "**Silhouette Score:**\n",
    "\n",
    "$$\n",
    "s = \\frac{b - a}{max(a, b)}\n",
    "$$\n",
    "\n",
    "- **a**: The mean distance between a sample and all other points in the same class.\n",
    "- **b**: The mean distance between a sample and all other points in the next nearest cluster.\n",
    "\n",
    "Silhouette Score for the dataset is the average computed as the average for all the examples.\n",
    "\n",
    "\n",
    "**Adjusted Rand Index (ARI):**\n",
    "\n",
    "$$\n",
    "ARI = \\frac{RI - Expected\\_RI}{max(RI) - Expected\\_RI}\n",
    "$$\n",
    "\n",
    "$$\n",
    "RI = \\frac{a + b}{a + b + c + d}\n",
    "$$\n",
    "\n",
    "\n",
    "- a, the number of pairs of elements in S that are in the same subset in X and in the same subset in Y\n",
    "- b, the number of pairs of elements in S that are in different subsets in X and in different subsets in Y\n",
    "- c, the number of pairs of elements in S that are in the same subset in X and in different subsets in Y\n",
    "- d, the number of pairs of elements in S that are in different subsets in X and in the same subset in Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh-yGfjfkjLE"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "silhouette_score_values = []\n",
    "adjusted_rand_score_values = []\n",
    "n_clusters = np.arange(2, 21)\n",
    "\n",
    "for n in n_clusters:\n",
    "    \n",
    "    clusterer = KMeans(n_clusters=n, max_iter=10, n_init=10)\n",
    "    clusterer.fit(X_blobs)\n",
    "    y_pred = clusterer.predict(X_blobs)\n",
    "    \n",
    "    score1 = metrics.silhouette_score(X_blobs, y_pred)\n",
    "    silhouette_score_values.append(score1)\n",
    "    \n",
    "    score2 = metrics.adjusted_rand_score(y_blobs, y_pred)\n",
    "    adjusted_rand_score_values.append(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryorKOQRPP8w",
    "outputId": "5604a5a6-77eb-4c25-a892-02444a0d538e"
   },
   "outputs": [],
   "source": [
    "y_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "EZpVVrFEkjLF",
    "outputId": "08f970cc-be3b-47d4-8085-5f2fa059c4cb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(n_clusters, silhouette_score_values, linewidth=3, label='Silhouette score')\n",
    "plt.plot(n_clusters, adjusted_rand_score_values, linewidth=3, label='Adjusted rand score')\n",
    "plt.xlabel('Number of clusters', size=16)\n",
    "plt.ylabel('Score', size=16)\n",
    "plt.xticks(n_clusters, size=16)\n",
    "plt.yticks(size=16)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbEsCa2_kjLF"
   },
   "source": [
    "## DBSCAN <a id=\"dbscan\"></a>\n",
    "\n",
    "<center><img src=\"dbscan.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JncitDvEkjLF"
   },
   "source": [
    "DBSCAN algorithm:\n",
    "- 1. Select a random point and find its neighbors in the given\n",
    "neighborhood\n",
    "- 2. If the neighbors are less than the critical value, we call them outliers\n",
    "- 3. If not, combine it into a \"dense\" cluster and repeat the search for neighbors\n",
    "- 4. If all dense points are passed and marked as visited-select a new point that is not visited and start over\n",
    "\n",
    "Cool visualisation of DBSCAN is here: https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "UHoL7XnakjLF",
    "outputId": "75b889fd-47ce-42c3-ae9b-e6e010cc92fc"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "n_samples = 150\n",
    "random_state = 170\n",
    "#X, y = datasets.make_blobs(centers=3, n_samples=n_samples, \n",
    "#random_state=random_state, center_box=(-10, 10))\n",
    "\n",
    "# To play with\n",
    "X, y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "# X, y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "# X = np.random.rand(n_samples, 2)\n",
    "\n",
    "plot_clusters(X, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "IqBy0N8LkjLG",
    "outputId": "51203227-d17e-448a-ab7e-94b830fefbe7"
   },
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "from sklearn import cluster\n",
    "\n",
    "# Run clusterer\n",
    "clusterer = cluster.DBSCAN(eps=0.25, min_samples=5)\n",
    "y_pred = clusterer.fit_predict(X)\n",
    "\n",
    "# Plot clustering results\n",
    "plot_clusters(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhZLLSjmkjLG"
   },
   "source": [
    "How to select parameters of DBSCAN algorithm properly? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Umo7acPkjLG"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "k = 5\n",
    "\n",
    "# estimate k neighbors for each point in X\n",
    "nn = NearestNeighbors(n_neighbors=k)\n",
    "nn.fit(X)\n",
    "\n",
    "# calculate distances for each of k nearest neighbors\n",
    "dists, _ = nn.kneighbors(X, n_neighbors=k)\n",
    "\n",
    "# take distaces only for the k-th neighbors\n",
    "dist_kth = dists[:, k-1]\n",
    "\n",
    "# sort the distances\n",
    "dist_kth.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "pqfUrAB-kjLH",
    "outputId": "b1cf09c5-77f7-4c2d-c6ec-ade5e4cdb92c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(dist_kth, linewidth=3, label='Distance to the k-th neighbor')\n",
    "plt.xlabel('Point number', size=16)\n",
    "plt.ylabel('Distance', size=16)\n",
    "plt.yticks(size=16)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4b3ybv-uEYK"
   },
   "source": [
    "##Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering is one of the popular and easy to understand clustering technique. \n",
    "\n",
    "This clustering technique is divided into two types:\n",
    "1. Agglomerative (the algorithm combines two smaller clusters into one at each iteration)\n",
    "2. Divisive (the algorithm splits one cluster into two smaller ones at each iteration)\n",
    "\n",
    "We will consider an agglomerative hierarchical clustering (divisive can be considered by analogy).\n",
    "\n",
    "**Agglomerative Hierarchical clustering Technique:** \n",
    "In this technique, initially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until one cluster or K clusters are formed.\n",
    "\n",
    "The basic algorithm of Agglomerative is straight forward.\n",
    "1. Compute the proximity matrix\n",
    "2. Let each data point be a cluster\n",
    "3. **Repeat:** Merge the two closest clusters and update the proximity matrix\n",
    "4. Until only a single cluster remains\n",
    "\n",
    "Resume:\n",
    "1. We can select any number of clusters stopping the algorithm when we need\n",
    "2. This algorithm is much less sensitive to the choice of metrics between points than other ones\n",
    "\n",
    "The Hierarchical clustering Technique can be visualized using a Dendrogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "bi_gmLOlm15J",
    "outputId": "51dd7bae-71bb-4e24-d34e-1c7a5c0cc287"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    # linkage matrix format: \n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.\n",
    "    # linkage.html#scipy.cluster.hierarchy.linkage\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:20]\n",
    "model = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.figure(figsize=(16,7))\n",
    "plot_dendrogram(model, labels=model.labels_)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UChlIeJAm82Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "oX-A_GFokjK5",
    "gVumNE28kjK7",
    "WoaRfevIkjK7",
    "hJ7hv1P0kjK9",
    "FxJXcyzkkjK_"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
