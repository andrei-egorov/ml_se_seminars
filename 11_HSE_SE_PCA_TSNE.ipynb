{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kk-xUDhbuilq"
   },
   "source": [
    "# Методы понижения размерности\n",
    "\n",
    "<b>Евгений Соколов</b>\n",
    "\n",
    "Довольно часто бывает так, что признаков очень много. Хочется уменьшить их число так, чтобы задача по-прежнему хорошо решалась. В этой тетрадке мы с вами посмотрим на несколько различных способов понижать размерность.\n",
    "\n",
    "> \"To deal with hyper-planes in a 14 dimensional space, visualize a 3D space and say 'fourteen' very loudly. Everyone does it.\" — Geoffrey Hinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sJy5uJMuilw"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXrJgKaouilx"
   },
   "source": [
    "# 1. Отбор признаков\n",
    "\n",
    "Самый простой способ выделения признаков &mdash; их отбор. Есть много разных стратегий отбора признаков. \n",
    "\n",
    "\n",
    "- __Одномерные стратегии:__ считаем насколько сильно признаки связаны с таргетом с помощью разных метрик, оставляем только самые связанные\n",
    "- __Жадные методы отбора признаков:__ надстройки над методами обучения моделей. Они перебирают различные подмножества признаков и выбирают то из них, которое дает наилучшее качество определённой модели машинного обучения. Данный процесс устроен следующим образом. Обучение модели считается черным ящиком, который на вход принимает информацию о том, какие из его признаков можно использовать при обучении модели, обучает модель, и дальше каким-то методом оценивается качество такой модели, например, по отложенной выборке или кросс-валидации. Таким образом, задача, которую необходимо решить, — это оптимизация функционала качества модели по подмножеству признаков. Признаки обычно перебираются по какому-то аллгоритму. Например, можно попробовать все комбинации (очень долго и неэффективно).\n",
    "- __Отбор признаков на основе моделей.__\n",
    "\n",
    "Отберем признаки на основе их корреляции с целевым признаком, и сравним результаты с исходными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "Koh2ZIKZuilz",
    "outputId": "87498071-0e36-441f-c04b-1328f222377a"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# сделаем тетрадку этичнее заменив load_boston()  на fetch_california_housing()\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "ds = fetch_california_housing()\n",
    "\n",
    "X, y = ds.data, ds.target\n",
    "indexes = np.arange(len(y))\n",
    "np.random.seed(52342)\n",
    "np.random.shuffle(indexes)\n",
    "X = X[indexes, :]\n",
    "y = y[indexes]\n",
    "\n",
    "features_ind = np.arange(X.shape[1])\n",
    "corrs = np.abs([pearsonr(X[:, i], y)[0] for i in features_ind])\n",
    "importances_sort = np.argsort(corrs)\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.barh(features_ind, corrs[importances_sort])\n",
    "plt.xlabel('importance', fontsize=20)\n",
    "X = X[:, importances_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8iB9U0tuil0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "features_counts = np.arange(X.shape[1])\n",
    "\n",
    "def scores_by_features_count(reg):\n",
    "    scores = []\n",
    "    for features_part in features_counts:\n",
    "        X_part = X[:,importances_sort[features_part:]]\n",
    "        scores.append(cross_val_score(reg, X_part, y).mean())\n",
    "    return scores\n",
    "\n",
    "linreg_scores = scores_by_features_count(LinearRegression())\n",
    "rf_scores = scores_by_features_count(RandomForestRegressor(n_estimators=100, max_depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "Y_AaW-pDuil1",
    "outputId": "a69a9a53-1f65-489d-ce6b-4a22fce2d9e1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.plot(features_counts, linreg_scores, label='LinearRegression')\n",
    "plt.plot(features_counts, rf_scores, label='RandomForest')\n",
    "plt.legend(loc='best', fontsize=20)\n",
    "plt.xlabel('#features deleted', fontsize=20)\n",
    "plt.ylabel('$R^2$', fontsize=20)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5kC0NCAuil2"
   },
   "source": [
    "В общем, если мы захотим немного сократить потребление ресурсов, пожертвовав частью качества,\n",
    "видно, что это можно сделать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPz5drzxuil3"
   },
   "source": [
    "# 2. Метод главных компонент (Principal Component Analysis, PCA)\n",
    "\n",
    "Выделение новых признаков путем их отбора часто дает плохие результаты, и в некоторых ситуациях такой подход практически бесполезен. Например, если мы работаем с изображениями, у которых признаками являются яркости пикселей,\n",
    "невозможно выбрать небольшой поднабор пикселей, который дает хорошую информацию о содержимом картинки. Поэтому признаки нужно как-то комбинировать.\n",
    "\n",
    "__Метод главных компонент__ &mdash; один из самых интуитивно простых и часто используемых методов для снижения размерности данных и проекции их на ортогональное подпространство признаков. В рамках метода делается два важных упрощения задачи\n",
    "\n",
    "1. игнорируется целевая переменная;\n",
    "2. строится линейная комбинация признаков.\n",
    "\n",
    "П. 1 на первый взгляд кажется довольно странным, но на практике обычно не является таким уж плохим. Это связано с тем, что часто данные устроены так, что имеют какую-то внутреннюю структуру в пространстве меньшей размерности, которая никак не связана с целевой переменной. Поэтому и оптимальные признаки можно строить не глядя на ответ.\n",
    "\n",
    "П. 2 тоже сильно упрощает задачу, но далее мы научимся избавляться от него."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcsASJ4Iuil3"
   },
   "source": [
    "### Теория\n",
    "\n",
    "Кратко вспомним, что делает этот метод (подробно см. в лекции).\n",
    "\n",
    "Пусть $X$ &mdash; матрица объекты-признаки, с нулевым средним каждого признака, а $w$ &mdash; некоторый единичный вектор. Тогда $Xw$ задает величину проекций всех объектов на этот вектор. Далее ищется вектор, который дает наибольшую дисперсию полученных проекций (то есть наибольшую дисперсию вдоль этого направления):\n",
    "\n",
    "$$\n",
    "\\max_{w: \\|w\\|=1} \\| Xw \\|^2 =  \\max_{w: \\|w\\|=1} w^T X^T X w\n",
    "$$\n",
    "\n",
    "Подходящий вектор тогда равен собственному вектору матрицы $X^T X$ с наибольшим собственным значением. После этого все пространство проецируется на ортогональное дополнение к вектору $w$ и процесс повторяется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJGHeSKVuil4"
   },
   "source": [
    "## 2.1 PCA на плоскости\n",
    "\n",
    "Для начала посмотрим на метод PCA на плоскости для того, чтобы лучше понять, как он устроен. Попробуем специально сделать один из признаков более значимым и проверим, что PCA это обнаружит. Сгенерируем выборку из двухмерного нормального распределения с нулевым математическим ожиданием. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0j0T6SM7uil4"
   },
   "outputs": [],
   "source": [
    "np.random.seed(314512)\n",
    "\n",
    "data_synth_1 = np.random.multivariate_normal(\n",
    "    mean=[0, 0], \n",
    "    cov=[[4, 0], \n",
    "         [0, 1]],\n",
    "    size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyP0H4D8uil4"
   },
   "source": [
    "Теперь изобразим точки выборки на плоскости и применим к ним PCA для нахождения главных компонент. В результате работы PCA из sklearn в `dec.components_` будут лежать главные направления (нормированные), а в `dec.explained_variance_` &mdash; дисперсия, которую объясняет каждая компонента. Изобразим на нашем графике эти направления, умножив их на дисперсию для наглядного отображения их значимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "HgLaaRjyuil5",
    "outputId": "0f55ff68-fe05-4179-f7ce-0b7ba3a2444c"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_show(dataset):\n",
    "    plt.scatter(*zip(*dataset), alpha=0.5)\n",
    "    \n",
    "    dec = PCA()\n",
    "    dec.fit(dataset)\n",
    "    ax = plt.gca()\n",
    "    for comp_ind in range(dec.components_.shape[0]):\n",
    "        component = dec.components_[comp_ind, :]\n",
    "        var = dec.explained_variance_[comp_ind]\n",
    "        start, end = dec.mean_, component * var\n",
    "        ax.arrow(start[0], start[1], end[0], end[1],\n",
    "                 head_width=0.2, head_length=0.4, fc='r', ec='r')\n",
    "    \n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euprFbbouil5"
   },
   "source": [
    "Видим, что PCA все правильно нашел. Но это, конечно, можно было сделать и просто посчитав\n",
    "дисперсию каждого признака. Повернем наши данные на некоторый фиксированный угол и проверим,\n",
    "что для PCA это ничего не изменит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "skVEV-vouil6",
    "outputId": "fa07a2a5-b487-42b9-d8b3-acda9935a24e"
   },
   "outputs": [],
   "source": [
    "angle = np.pi / 6\n",
    "rotate = np.array([\n",
    "        [np.cos(angle), - np.sin(angle)],\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "    ])\n",
    "data_synth_2 = rotate.dot(data_synth_1.T).T\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfbQz3Lhuil6"
   },
   "source": [
    "Ниже пара примеров, где PCA отработал не так хорошо (в том смысле, что направления задают не очень хорошие признаки).\n",
    "\n",
    "**Упражнение:** объясните, почему так произошло."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "rbyVfQ5Puil6",
    "outputId": "3d6a400c-79e1-4fc6-dae8-c880cc5b3567",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_moons, make_blobs\n",
    "\n",
    "np.random.seed(54242)\n",
    "\n",
    "data_synth_bad = [\n",
    "    make_circles(n_samples=1000, factor=0.2, noise=0.1)[0]*2,\n",
    "    make_moons(n_samples=1000, noise=0.1)[0]*2,\n",
    "    make_blobs(n_samples=1000, n_features=2, centers=4)[0]/5,\n",
    "    np.random.multivariate_normal(\n",
    "        mean=[0, 1.5], \n",
    "        cov=[[3, 1], \n",
    "             [1, 1]],\n",
    "        size=1000),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "rows, cols = 2, 2\n",
    "for i, data in enumerate(data_synth_bad):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    PCA_show(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsxGXRAguil7"
   },
   "source": [
    "## 2.2 Лица людей\n",
    "\n",
    "Рассмотрим датасет с фотографиями лиц людей и применим к его признакам PCA. Ниже изображены примеры лиц из базы, и последняя картинка &mdash; это \"среднее лицо\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "Y8mUGTKtuil7",
    "outputId": "f1325ae3-df9b-4463-be23-f42039de50d7"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "faces = fetch_olivetti_faces(shuffle=True, random_state=432542)\n",
    "faces_images = faces.data\n",
    "faces_ids = faces.target\n",
    "image_shape = (64, 64)\n",
    "    \n",
    "mean_face = faces_images.mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "rows, cols = 2, 4\n",
    "n_samples = rows * cols\n",
    "for i in range(n_samples - 1):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(faces_images[i, :].reshape(image_shape), interpolation='none',\n",
    "               cmap='gray')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "plt.subplot(rows, cols, n_samples)\n",
    "plt.imshow(mean_face.reshape(image_shape), interpolation='none',\n",
    "           cmap='gray')\n",
    "plt.xticks(())\n",
    "_ = plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwAGKB8huil7"
   },
   "source": [
    "Теперь найдем главные компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "X146vBs2uil7",
    "outputId": "ff8d0861-4e8a-4e33-bafa-5da30d48420b"
   },
   "outputs": [],
   "source": [
    "model_pca = PCA()\n",
    "faces_images -= mean_face  # отнормировали данные к нулевому среднему\n",
    "model_pca.fit(faces_images)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "rows, cols = 2, 4\n",
    "n_samples = rows * cols\n",
    "for i in range(n_samples):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(model_pca.components_[i, :].reshape(image_shape), interpolation='none', cmap='gray')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftqUOZN1uil8"
   },
   "source": [
    "Получилось жутковато, что уже неплохо, но есть ли от этого какая-то польза?\n",
    "\n",
    "- Во-первых, новые признаки дают более высокое качество классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P49S722Tuil8"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gscv_rf = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    {'n_estimators': [100, 200, 500, 800], 'max_depth': [2, 3, 4, 5]}, cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hw2rBOI8uil8",
    "outputId": "6dad8342-26aa-4503-adb1-47bf7eed0c88"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gscv_rf.fit(faces_images, faces_ids)\n",
    "print(gscv_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRG4Cnyhuil8",
    "outputId": "18ee889b-e77e-4a82-fdda-cf2bf28ad2e8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gscv_rf.fit(model_pca.transform(faces_images)[:,:100], faces_ids)\n",
    "print(gscv_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8H_CqZLuil9"
   },
   "source": [
    "На практике можно выбирать столько главных компонент, чтобы оставить $90\\%$ дисперсии исходных данных. В данном случае для этого достаточно выделить около $60$ главных компонент, то есть снизить размерность с $4096$ признаков до $60$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yCEpxrAuil9",
    "outputId": "0825c87f-e432-484d-e639-200244f7c4f1"
   },
   "outputs": [],
   "source": [
    "faces_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "oJVPu3B1uil9",
    "outputId": "12a58245-960b-4def-d467-8d1f2db108df"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(model_pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, 63)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.axvline(21, c='b')\n",
    "plt.axhline(0.9, c='r')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH9FhLMuuil9"
   },
   "source": [
    " - Во-вторых, их можно использовать для компактного хранения данных. Для этого объекты трансформируются в новое пространство, и из него выкидываются самые незначимые признаки. Ниже приведены результаты сжатия в 20 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "EKPJL0_Duil-",
    "outputId": "fa84e0b2-5a29-4c16-ddd6-2a49622d86b6"
   },
   "outputs": [],
   "source": [
    "base_size = image_shape[0] * image_shape[1]\n",
    "\n",
    "def compress_and_show(compress_ratio):\n",
    "    model_pca = PCA(n_components=int(base_size * compress_ratio))\n",
    "    model_pca.fit(faces_images)\n",
    "\n",
    "    faces_compressed = model_pca.transform(faces_images)\n",
    "    \n",
    "    # обратное преобразование\n",
    "    faces_restored = model_pca.inverse_transform(faces_compressed) + mean_face\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    rows, cols = 2, 4\n",
    "    n_samples = rows * cols\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(faces_restored[i, :].reshape(image_shape), interpolation='none',\n",
    "                   cmap='gray')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "compress_and_show(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "528KQS_Zuil-"
   },
   "source": [
    "И даже при сжатии в 50 раз лица остаются узнаваемыми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "MgowiDZMuil-",
    "outputId": "b331482a-65c3-439e-be81-1fb16f40bf88"
   },
   "outputs": [],
   "source": [
    "compress_and_show(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFpuIZ2cuil_"
   },
   "source": [
    "## 2.3 PCA для визуализации химического состава рек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8CouQZruil_"
   },
   "source": [
    "Посмотрим на ещё один пример применения метода главных компонент. Будем работать с подмножеством [датасета о химическом составе воды](http://data.europa.eu/euodp/en/data/dataset/data_waterbase-rivers-10) в разных реках.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xA2ujy9gvQlf",
    "outputId": "73d883ef-cab1-42a5-b0f1-65e111dd22c5"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/FUlyankin/Intro_to_DS/raw/master/data/water_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39ichZB4uil_"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "index_list, feature_list, data_matrix = pickle.load(open('water_dataset', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuO9D2OSuil_"
   },
   "source": [
    "* `index_list` - список id рек, которые были отобраны для задания\n",
    "* `feature_list` - список признаков (они имеют вид `год ПРОБЕЛ показатель`)\n",
    "* `data_matrix` - собственно данные (строки соответствуют рекам из `index_list`, а столбцы - признакам из `feature_list`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi2yhpfAuil_"
   },
   "source": [
    "Давайте попробуем сделать PCA, визуализровать данные в простанстве первых двух компонент и проинтерпретировать первые главные компоненты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBU5Szf0uil_",
    "outputId": "065fb099-04e2-4d32-89ec-a2b45c8d43e9"
   },
   "outputs": [],
   "source": [
    "X = data_matrix.copy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "YrlBd5ZruimA",
    "outputId": "a4439558-ce32-425d-b5ed-166840b37899"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model_pca = PCA(10) # оставим 10 компонент\n",
    "X_pca = model_pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGC_UElsuimA"
   },
   "source": [
    "Прямое применение PCA  к данным не дало хорошегго результата. \n",
    "\n",
    "- Вспомним, что PCA пытается выделять главные компоненты, максимизируя дисперсию. Дисперсия чувствительна к выбросам, значит и метод главных компонент чувствителен к ним. \n",
    "- Судя по всемму у данных разные единицы измерения, значит разброс несопоставим. PCA запутался."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "nUKZ5ItcuimA",
    "outputId": "47a94047-9ed1-4521-9040-2fdd376e22f6"
   },
   "outputs": [],
   "source": [
    "plt.hist(X.max(axis=0), bins=30,log=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxUpDaKpuimA"
   },
   "source": [
    "Срежем выбросы по $99\\%$ квантилю, а затем стандартизируем данные. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBZm7O6TuimA"
   },
   "outputs": [],
   "source": [
    "replace = np.percentile(X, 99, axis=0)\n",
    "for i in range(X.shape[1]):\n",
    "    X[:,i][X[:,i] > replace[i]] = replace[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "_c40zYWauimA",
    "outputId": "83b4d263-9e66-4313-8ae6-7ba247f3a07a"
   },
   "outputs": [],
   "source": [
    "plt.hist(X.max(axis=0), bins=30,log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6mLc_CruimB"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scal = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "qrjGsAeauimB",
    "outputId": "32a3feb0-7829-4292-9bdf-3f62285d7232"
   },
   "outputs": [],
   "source": [
    "model_pca = PCA(10)\n",
    "X_pca = model_pca.fit_transform(X_scal)\n",
    "\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aQ7iwpwuimB"
   },
   "source": [
    "Визуализация стала лучше. Чтобы понять физический смысл первых признаков, посмотрим из каких колонок они формируются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQ2clqn1uimB",
    "outputId": "fa3a1f28-6019-48a3-bbb7-f89eddaddfb3"
   },
   "outputs": [],
   "source": [
    "sorted(list(zip(feature_list, np.abs(model_pca.components_[0]))), key = lambda w : w[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YJ3UgXVuimB",
    "outputId": "24fe78c3-bf4e-40fa-8681-5a1fe57e9393"
   },
   "outputs": [],
   "source": [
    "sorted(list(zip(feature_list, np.abs(model_pca.components_[1]))), key = lambda w : w[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VX3S-oVjuimB",
    "outputId": "f8cd6a85-f20a-4951-9493-b2a7c50d9cdb"
   },
   "outputs": [],
   "source": [
    "sorted(list(zip(feature_list, np.abs(model_pca.components_[3]))), key = lambda w : w[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7aP8Z6uuimC"
   },
   "source": [
    "## 2.4 PCA с ядрами\n",
    "\n",
    "Так как PCA фактически работает не исходными признаками, а с матрицей их ковариаций, можно использовать для ее вычисления вместо скалярного произведения $\\langle x_i, x_j \\rangle$ произвольное ядро $K(x_i, x_j)$. Это будет соответствовать переходу в другое пространство. Единственная проблема &mdash; непонятно, как подбирать ядро.\n",
    "\n",
    "Ниже приведены примеры объектов в исходном пространстве (похожие группы обозначены одним цветом для наглядности), и результат их трансформации в новые пространства (для разных ядер). Если результаты получаются линейно разделимыми &mdash; значит мы выбрали подходящее ядро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "-QBnDEuLuimC",
    "outputId": "517480c8-2859-42ea-e8a7-93f11c4123f2"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "def KPCA_show(X, y):\n",
    "    reds = y == 0\n",
    "    blues = y == 1\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    rows, cols = 2, 2\n",
    "    plt.subplot(rows, cols, 1)\n",
    "    plt.scatter(X[reds, 0], X[reds, 1], alpha=0.5, c='r')\n",
    "    plt.scatter(X[blues, 0], X[blues, 1], alpha=0.5, c='b')\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    kernels_params = [\n",
    "        dict(kernel='rbf', gamma=10),\n",
    "        dict(kernel='poly', gamma=10),\n",
    "        dict(kernel='cosine', gamma=10),\n",
    "    ]\n",
    "    \n",
    "    for i, p in enumerate(kernels_params):\n",
    "        dec = KernelPCA(**p)\n",
    "        X_transformed = dec.fit_transform(X)\n",
    "        \n",
    "        plt.subplot(rows, cols, i + 2)\n",
    "        plt.scatter(X_transformed[reds, 0], X_transformed[reds, 1], alpha=0.5, c='r')\n",
    "        plt.scatter(X_transformed[blues, 0], X_transformed[blues, 1], alpha=0.5, c='b')\n",
    "        ax = plt.gca()\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        \n",
    "np.random.seed(54242)\n",
    "KPCA_show(*make_circles(n_samples=1000, factor=0.2, noise=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "EwWGbRobuimC",
    "outputId": "b45aac4d-b08d-4e3c-d0f9-a523231d7b06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(54242)\n",
    "KPCA_show(*make_moons(n_samples=1000, noise=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-I_rLx4uimC"
   },
   "source": [
    "# 3. TSNE (t-distributed Stohastic Neighbor Embedding)\n",
    "\n",
    "Джефри Хинтон не просто сказал цитату из эпирафа к этой тетрадке, но и вместе со своим аспирантом, в 2008 году, придумал [новый методв изуализации данных.](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) Основная идея метода состоит в поиске отображения из многомерного признакового пространства на плоскость (или в 3D, но почти всегда выбирают 2D), чтоб точки, которые были далеко друг от друга, на плоскости тоже оказались удаленными, а близкие точки – также отобразились на близкие. То есть neighbor embedding – это своего рода поиск нового представления данных, при котором сохраняется соседство.\n",
    "\n",
    "Попробуем взять данные о рукописных цифрах и визуализируем их с помощью PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "swG1oy__uimC",
    "outputId": "28d01bfd-48f4-445e-9b4e-5497b1900bbb"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X[i,:].reshape([8,8]), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnCpjWx0uimD",
    "outputId": "ac201e69-bf6c-4c07-cbb0-66099706ef14"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Q8W75IuimD"
   },
   "source": [
    "Получается, размерность признакового пространства здесь – 64. Но давайте снизим размерность всего до 2 и увидим, что даже на глаз рукописные цифры неплохо разделяются на кластеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "Dm3E1BupuimD",
    "outputId": "4759fbc3-cbb5-4c84-e931-0f6b49c959f4"
   },
   "outputs": [],
   "source": [
    "model_pca = PCA(n_components=2)\n",
    "X_reduced = model_pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. PCA projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFbON5pduimD"
   },
   "source": [
    "Попробуем сделать то же самое с помощью t-SNE. Картинка получится лучше, так как у PCA есть существенное ограничение - он находит только линейные комбинации исходных признаков (если не добавить какое-нибудь ядро).  Внутри sklearn есть реализация TSNE, но она не такая эффективная как библиотека [MulticoreTSNE,](https://github.com/DmitryUlyanov/Multicore-TSNE) в которой метод можно распараллелить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cmake==3.18.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1aaF0sbuimD",
    "outputId": "435e646c-5709-46ae-b2f4-b1c557637bc8"
   },
   "outputs": [],
   "source": [
    "pip install MulticoreTSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "3Vtyf6nsuimE",
    "outputId": "af24892f-abf5-41ca-cf4a-d445a1a6e999"
   },
   "outputs": [],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "tsne = TSNE(n_jobs=4, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. t-SNE projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YU1sW2TEyXx9"
   },
   "source": [
    "У метода есть параметр `Perplexity`, который отвечает за то, насколько сильно точки могут разлететься друг от друга. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "PUeSX7fRyWY3",
    "outputId": "4b11f3f5-88d8-45d1-913a-bc6032986b75"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_jobs=4, perplexity=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. t-SNE projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3Xpshj6xkHF"
   },
   "source": [
    "Итоговая картинка может сильно поменяться при изменении `random_state`, это усложняет интерпретацию. В целом, по таким картинкам не стоит делать далеко идущих выводов – не стоит гадать по кофейной гуще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H4uyY8SzgGx"
   },
   "source": [
    "В 2018 году был предложен ещё один алгоритм нелинейного снижения размерности, [UMAP.](https://umap-learn.readthedocs.io/en/latest/) Он похож на TSNE, но работает быстрее и более эффективен. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfOL4loiuimF"
   },
   "source": [
    "- [статья](https://habr.com/ru/company/io/blog/265089/) \"Как подобрать платье с помощью метода главных компонент\"\n",
    "- [Q&A Разбор PCA с интуицией и примерами](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n",
    "- [Distillpub о TSNE](https://distill.pub/2016/misread-tsne/)\n",
    "- [Подробнее про UMAP](https://pair-code.github.io/understanding-umap/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sem12_pca_tsne.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
