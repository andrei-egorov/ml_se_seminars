{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcXyA18WP551"
   },
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Seminar 4: Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkpzLhVjP557"
   },
   "source": [
    "**Authors**: Polina Polunina, Yury Kashnitsky, Maria Tikhonova, Eugeny Sokolov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL8pCd9hP55-"
   },
   "source": [
    "Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aByHV_4KP55-",
    "outputId": "7050b042-5383-47d0-b13a-5d5cabca5d8b"
   },
   "outputs": [],
   "source": [
    "#linear algebra\n",
    "import numpy as np\n",
    "#data structures\n",
    "import pandas as pd\n",
    "#ml models\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#beautiful plots\n",
    "import seaborn as sns\n",
    "#linear regression\n",
    "import statsmodels.api as sm\n",
    "#set style for plots\n",
    "sns.set_style('darkgrid')\n",
    "#ulr links\n",
    "import wget\n",
    "#off the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX74u3XxP55_"
   },
   "source": [
    "## Simple Linear Regression\n",
    "Simple Linear Regression - regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OISPz3RhP55_"
   },
   "source": [
    "Load the dataset:\n",
    "* **SalePrice** - The property's sale price in dollars. This is the target variable that you're trying to predict\n",
    "* **GrLivArea** - Above grade (ground) living area square feet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "YbqllKenP56A",
    "outputId": "ca968a3c-8754-4a9b-d632-f781dc7171fb"
   },
   "outputs": [],
   "source": [
    "# store the web link in a variable\n",
    "url = 'https://docs.google.com/uc?export=download&id=1k21iUIrz0NjfiLE_j-oBQm1bNu3wASX6'\n",
    "\n",
    "# download the file and save its name to a viriable\n",
    "filename = wget.download(url)\n",
    "\n",
    "# print the filename\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bF-Jt4Y9P56C"
   },
   "outputs": [],
   "source": [
    "# loading the data from pc\n",
    "data = pd.read_csv(filename, index_col=0, usecols=['Id', 'GrLivArea', 'SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etb8lQHqP56C"
   },
   "source": [
    "Let's have a look into the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "cfzKgEz7P56D",
    "outputId": "115488a5-a8e2-4f8b-bb11-1fade48462da"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "-Q9woeT3P56E",
    "outputId": "918ca9bd-83b2-4595-924e-093077bdfcc6"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "SrOzT853P56E",
    "outputId": "0705daa5-d99e-457d-f6e6-9dbc6d641243"
   },
   "outputs": [],
   "source": [
    "data.GrLivArea.hist(bins=20, label='GrLivArea')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "As1qJaLdP56E",
    "outputId": "0868487e-8cef-4b51-95a1-3af712721de6"
   },
   "outputs": [],
   "source": [
    "data.SalePrice.hist(bins=30, label='SalePrice')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsvrF2xNP56G",
    "outputId": "cdfee2cc-899e-450e-8a60-d2405984f5ca"
   },
   "outputs": [],
   "source": [
    "data.SalePrice.quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "tbcDAXEOP56G",
    "outputId": "9898d0e2-3e8e-491b-f647-5ab38b15fdb1"
   },
   "outputs": [],
   "source": [
    "#set figsize of the plot\n",
    "plt.figure(figsize = (10,7))\n",
    "#scatter plot of the data\n",
    "plt.scatter(data.GrLivArea, data.SalePrice)\n",
    "#text for x axis\n",
    "plt.xlabel('Living area, square feet')\n",
    "#text for y axis\n",
    "plt.ylabel('Sale Price, dollars')\n",
    "#text for the plot title\n",
    "plt.title('Dependence between House Sale Price and Living Area')\n",
    "#show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_4EmgTyP56G"
   },
   "source": [
    "### How to model this dependence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y3kER0dP56G"
   },
   "source": [
    "### Building a model\n",
    "\n",
    "* Y = SalePrice - target, dependent variable \n",
    "* X = GrLivArea - predictor, independent variable\n",
    "\n",
    "**the model**\n",
    "\n",
    "We want to find a line that reflects the dependence between Sale Price and Living area\n",
    "\n",
    "\n",
    "$Y = a + bX + \\epsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewjvLswUP56G"
   },
   "outputs": [],
   "source": [
    "X = data.GrLivArea\n",
    "Y = data.SalePrice\n",
    "#add the constant term to the data\n",
    "X = sm.add_constant(X)\n",
    "#define the model\n",
    "model = sm.OLS(Y, X)\n",
    "#fit the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "l-jvpbgIP56H",
    "outputId": "b3726d28-d406-4fa5-a47f-74d44e41d79e"
   },
   "outputs": [],
   "source": [
    "#plot the summary of our model\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jb7I1Q_P56I"
   },
   "source": [
    "### What do all these stats mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJFEDWvVP56I"
   },
   "source": [
    "* **const** - the found value for a\n",
    "* **GrLivArea** - the found value for b\n",
    "\n",
    "so that our model is **Y = 18569.0259 + 107.1304 * X**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHZkoLLhP56I"
   },
   "source": [
    "**Hypothesis testing and *p-value***\n",
    "\n",
    "$H_0$: coeff = 0 - null hypothesis\n",
    "\n",
    "$H_1$: coeff $\\neq$ 0 - alternative hypothesis\n",
    "\n",
    "* If *p-value* $\\leq$ alpha, then we **CAN** reject the null hypothesis and the coeff is called **significant**\n",
    "* If *p-value* > alpha, then we **CAN NOT** reject the null hypothesis and the coeff is called **insignificant**\n",
    "    \n",
    "    \n",
    "**How to choose a suitable alpha value?**\n",
    "\n",
    "Alphas could be: 0.01, 0.05, 0.1 ... \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1hOljO6iysu6VPtctoRjZVole2qEnTidB\" width=70%>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1yEiawA_wtaPYIb6n1uEoV8bfw_6ZVTwx\" width=70%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R98AHV4UP56I"
   },
   "source": [
    "What is the appropriate false positives and false negatives level?\n",
    "\n",
    "The most common alpha for coeffs = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFE1ou1RP56K"
   },
   "source": [
    "#### How well this relation reflects the dependence?\n",
    "**Y = 18569.0259 + 107.1304 * X**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "nuNJf6jrP56K",
    "outputId": "08782b04-3c54-4454-8630-d840d242a9ef"
   },
   "outputs": [],
   "source": [
    "#set size of the plot\n",
    "plt.figure(figsize = (10,7))\n",
    "#scatter plot of the data\n",
    "plt.scatter(data.GrLivArea, data.SalePrice, alpha=0.6, label='Original data')\n",
    "#plot of the found regression line\n",
    "plt.plot(\n",
    "    data.GrLivArea.values, \n",
    "    18569.0259 + 107.1304 * data.GrLivArea.values, \n",
    "    color='orchid', \n",
    "    label='Model'\n",
    ")\n",
    "\n",
    "#text for x axis\n",
    "plt.xlabel('Living area, square feet')\n",
    "#text for y axis\n",
    "plt.ylabel('Sale Price, dollars')\n",
    "#text for the plot title\n",
    "plt.title('Dependence between House Sale Price and Living Area')\n",
    "plt.legend()\n",
    "#show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZz4LQ71P56K"
   },
   "source": [
    "#### $R^2$ and Regression Performance\n",
    "\n",
    "Another recall from you statistics course =)\n",
    "\n",
    "$R^2$ is the **coefficient of determination**, the most common performance metric for regression problems\n",
    "\n",
    "In case of linear regression, $R^2$ is defined in the following way:\n",
    "\n",
    "* $y_i$ - observed target data\n",
    "* $\\hat{y_i}$ - predicted data\n",
    "* $\\overline{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i$ - mean of the observed data\n",
    "* $SS_{tot} = \\sum_{i}(y_i - \\overline{y})^2$ - total sum of squares\n",
    "* $SS_{reg} = \\sum_{i}(\\hat{y_i} - \\overline{y})^2$ - explained sum of squares\n",
    "* $SS_{res} = \\sum_{i}(y_i - \\hat{y_i})^2 = \\sum_{i}residual_i^2$ - residual sum of squares\n",
    "\n",
    "$R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}}$ - the ratio of the explained variance \n",
    "\n",
    "$R^2$ range is [0, 1]\n",
    "\n",
    "In our model, $R^2 = 0.502$, so only a half of the variance is explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuuOsaSTP56K"
   },
   "source": [
    "**Note:** $R^2$ is biased (!) and we should look into $R^2_{adjusted}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rI7grhpAP56K"
   },
   "source": [
    "### Can we do better?\n",
    "\n",
    "Let's take logarithms from X and Y, so that our model is **$ln(Y) = a + b*ln(X) + \\epsilon$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3llkzCMlP56K"
   },
   "outputs": [],
   "source": [
    "X = data.GrLivArea\n",
    "Y = data.SalePrice\n",
    "X=np.log(X)\n",
    "Y=np.log(Y)\n",
    "#add the constant term to the data\n",
    "X = sm.add_constant(X)\n",
    "#define the model\n",
    "model = sm.OLS(Y, X)\n",
    "#fit the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "a7W6uOYFP56L",
    "outputId": "11c46fc5-8846-4719-9e7c-2b1cb74a4a1e"
   },
   "outputs": [],
   "source": [
    "#plot the summary of our model\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "ljuuybYJP56L",
    "outputId": "f6198875-164b-4e4c-c958-3b940944b21d"
   },
   "outputs": [],
   "source": [
    "#set size of the plot\n",
    "plt.figure(figsize = (10,7))\n",
    "#scatter plot of the data\n",
    "plt.scatter(np.log(data.GrLivArea), np.log(data.SalePrice), alpha=0.6, label = 'Original data')\n",
    "#plot of the found regression line\n",
    "plt.plot(np.log(data.GrLivArea.values), 5.6681 + 0.8745 * np.log(data.GrLivArea.values), color = 'orchid', label='Model')\n",
    "#text for x axis\n",
    "plt.xlabel('log(Living area), square feet')\n",
    "#text for y axis\n",
    "plt.ylabel('log(Sale Price), dollars')\n",
    "#text for the plot title\n",
    "plt.title('Dependence between log(House Sale Price) and log(Living Area)')\n",
    "plt.legend()\n",
    "#show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA5DKp7gP56L"
   },
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-AaKWuGP56L"
   },
   "outputs": [],
   "source": [
    "cols=['Id', 'MSSubClass', 'LotArea', 'OverallQual',\\\n",
    "      'OverallCond', 'YearBuilt', 'YearRemodAdd',\\\n",
    "      'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\\\n",
    "     'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\\\n",
    "     'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\\\n",
    "     'TotRmsAbvGrd', 'Fireplaces', 'GarageCars',\\\n",
    "     'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\\\n",
    "     'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "85CM7ZR2P56M",
    "outputId": "f47e3eff-8d47-4418-9b7d-81c2698a4b7e"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename, index_col=0, usecols=cols)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXDwt5amP56N"
   },
   "source": [
    "Check for Nan values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z-Y1YlWhP56N",
    "outputId": "b87e7f34-b83f-4a07-f32d-a30062bf00b9"
   },
   "outputs": [],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTNy4bhMP56N"
   },
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sm2hl0kKP56N"
   },
   "outputs": [],
   "source": [
    "X = data.drop('SalePrice', axis=1)\n",
    "Y = data.SalePrice\n",
    "#add the constant term to the data\n",
    "X = sm.add_constant(X)\n",
    "#define the model\n",
    "model = sm.OLS(Y, X)\n",
    "#fit the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp0ut9liP56N"
   },
   "source": [
    "Calculate some statistical parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChlwMg8qP56N",
    "outputId": "ce719a2d-6bd2-44e6-ef53-c5e668614f8c"
   },
   "outputs": [],
   "source": [
    "#together with the intercept\n",
    "k = X.shape[1]\n",
    "#total number of observations\n",
    "n = X.shape[0]\n",
    "#degrees of freedom for the model:\n",
    "df_model = k - 1\n",
    "#degrees of freedom of the error:\n",
    "df_error = n - k\n",
    "print(' the number of paraters to estimate: {} \\n total number of observations: {} \\n degrees of freedom of the model: {} \\n degrees of freedom of the errors: {}'\\\n",
    "      .format(k, n, df_model, df_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqJpay2bP56N"
   },
   "source": [
    "Calculate the rank of feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnosVATGP56N",
    "outputId": "18781d3b-fd80-4d60-a818-2c01bb731862"
   },
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "id": "AtMo9gEcP56O",
    "outputId": "cf835ff3-7555-44a4-bbb8-5f04831ae7bf"
   },
   "outputs": [],
   "source": [
    "results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCzI1gQxP56O"
   },
   "source": [
    "**Resuls explanation:**\n",
    "\n",
    "* $R^2 = 0.808$\n",
    "* $R^2_{adj} = 0.804$\n",
    "* Log-Likelihood = -1733 - A value of Likelihood function in the optimal point\n",
    "* AIC = 34738 - Akaike information criterion, is used for model selection purposes. Preferred model is the one with the minimum AIC value\n",
    "* BIC = 34897 - Bayesian information criterion, the same purposes as for AIC\n",
    "* F-statistic = 208\n",
    "\n",
    "**F- Test for the overall model significance**:\n",
    "\n",
    "$H_{0}$ : The fit of intercept only model and the current model is same. i.e. Additional variables do not provide value taken together\n",
    "\n",
    "$H_{1}$ : The fit of intercept only model is significantly less compared to our current model. i.e. Additional variables do make the model significantly better.\n",
    "\n",
    "$F = \\frac{R^2/(k-1)}{(1-R^2)/(n-k)}$,\n",
    "\n",
    "where $k$ - the number of variables (with intercept term), $n$ - the number of observations\n",
    "\n",
    "\n",
    "If the calculated F-value is greater than the F value from the statistical table, than we can reject the $H_{0}$ hypothesis\n",
    "\n",
    "* Prob(F-statistic) = 0.0 - P-value for F-test\n",
    "* Df model - degrees of freedom of the model\n",
    "* Df Residuals - degrees of freedom of the errors\n",
    "\n",
    "**Note:** don't be confused with the values of Df (!). THe true values is calculated above. In this specific realization of OLS, DF model is calculated as a rank of the X matrix, which equals to 29\n",
    "\n",
    "* Scale - squared standard error of the regression\n",
    "\n",
    "* Durbin-Watson = 1.96; DW is a test for autocorrelation of the errors. DW value always lies between 0 and 4. If , DW << 2 there is a positive serial correlation, if DW >> 2 - there is a negative correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Src83oP56O"
   },
   "source": [
    "**Note:** not all the variables are significant. What should we do next? There is a number of methods (a.k.a. Feature engineering):\n",
    "\n",
    "* **Elimination by P-value:**\n",
    "\n",
    "Build a model using a full set of features. Then, eliminate the insignificant features sequentially starting from the one with the highest P-value\n",
    "\n",
    "* **Forward elimination:**\n",
    "\n",
    "Build all possible regression models with a single predictor and pick the best one. Then try all possible models that include that best predictor plus a second predictor. Pick the best of those. You keep adding one feature at a time, and you stop when your model no longer improves or starts worsening. \n",
    "\n",
    "* **Backward elimination:**\n",
    "\n",
    "Build a regression model that includes a full set of predictors. Next, gradually remove one at a time according to the predictor whose removal makes the biggest improvement. You stop removing predictors when the removal makes the predictive model worsen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ml5z_maPP56P"
   },
   "source": [
    "## Multiple regression with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0yfiLQfP56Q"
   },
   "source": [
    "* [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) &mdash; \"classical\" linear regression with MSE. Exact solution: $w^* = (X^TX)^{-1}X^Ty$\n",
    "* [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) &mdash; linear regression with MSE optimization and $\\ell_2$-regularization\n",
    "* [`Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) &mdash; linear regression with MSE optimization and $\\ell_1$-regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Gpx-oSwP56Q"
   },
   "source": [
    "## Regularization\n",
    "\n",
    "* Insert additional requirement for regularizer $R(\\beta)$ to be small:\n",
    "$$\n",
    "\\sum_{n=1}^{N}\\left(x_{n}^{T}\\beta-y_{n}\\right)^{2}+\\lambda R(\\beta)\\to\\min_{\\beta}\n",
    "$$\n",
    "* $\\lambda>0$ - hyperparameter.\n",
    "* $R(\\beta)$ penalizes complexity of models.\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "R(\\beta)=||\\beta||_{1} &  \\mbox{(L1) Lasso regression}\\\\\n",
    "R(\\beta)=||\\beta||_{2}^{2} & \\text{(L2) Ridge regression}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1WIONg5WAtiV4jKjmOA2Zn_B4uPAVPp6y\" width=70%>\n",
    "\n",
    "* Not only **accuracy** matters for the solution but also **model simplicity**!\n",
    "* $\\lambda$ controls complexity of the model:$\\uparrow\\lambda\\Leftrightarrow\\text{complexity}$$\\downarrow$.\n",
    "\n",
    "\n",
    "### Types of regularization: Ridge regression\n",
    "\n",
    "Ridge regression minimizes a slightly different function:\n",
    "\n",
    "$$\\Large J(X, y, \\beta) = \\mathcal{L} + \\lambda \\sum_j\\beta_j^2$$\n",
    "\n",
    "- $\\mathcal{L}$ is the logistic loss function summed over the entire dataset\n",
    "- $\\lambda \\sum_j\\beta_j^2$ is calles $l_2$ penalty\n",
    "\n",
    "\n",
    "where $\\lambda ≥ 0$ is a tuning parameter, to be determined separately. As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small. However, the second term, $\\lambda |\\beta|^2$ , called a *shrinkage penalty*, is small when betas are close to zero, and so it has the effect of shrinking penalty the estimates of $\\beta_j$ towards zero. The tuning parameter $\\lambda$ serves to control\n",
    "the relative impact of these two terms on the regression coefficient estimates. When $\\lambda = 0$, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as $\\lambda$ increases the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero.\n",
    "\n",
    "### Types of Regularization: Lasso regression\n",
    "\n",
    "Ridge regression does have one obvious disadvantage. It does not do feature selection. Ridge regression\n",
    "will include all p predictors in the final model. The penalty $\\lambda |\\beta|^2$ will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero. This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large. \n",
    "\n",
    "The lasso is an alternative to ridge regression that overcomes this disadvantage. The lasso minimizes the following function:\n",
    "\n",
    "\n",
    "$$\\Large J(X, y, \\beta) = \\mathcal{L} + \\lambda \\sum_j|\\beta_j|$$\n",
    "\n",
    "- $\\mathcal{L}$ is the logistic loss function summed over the entire dataset\n",
    "- $\\lambda \\sum_j|\\beta_j|$ is calles $l_1$ penalty\n",
    "\n",
    "The lasso and ridge regression have similar formulations. The only difference is that the $\\lambda\\sum_j\\beta_j^2 $ term in the ridge regression penalty has been replaced by $\\lambda\\sum_j|\\beta_j|$ in the lasso penalty.\n",
    "\n",
    "As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when\n",
    "the tuning parameter $\\lambda$ is sufficiently large. Hence, much like best subset selection, the lasso performs *variable selection*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAE7VV1tP56Q",
    "outputId": "68109d2a-e544-4335-c24a-e28cf3d73b26"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGWemqI1P56R"
   },
   "source": [
    "#### L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVeZvyt4P56S",
    "outputId": "7a939f2c-5a4a-4bd0-ccb1-583f2f8cbf1d"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop('SalePrice', axis=1), data.SalePrice, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "print(\"Test MSE = %.4f\" % mean_squared_error(y_test, y_pred))\n",
    "print(\"Train MSE = %.4f\" % mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBHxCMHBP56S"
   },
   "source": [
    "### Cross-Validation\n",
    "\n",
    "<img src=\"https://docs.splunk.com/images/thumb/e/ee/Kfold_cv_diagram.png/1200px-Kfold_cv_diagram.png\" width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkS1f-9bP56S",
    "outputId": "ef812496-c531-4c08-822d-69d35fff6923"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation scores:\\n\\t\", \"\\n\\t\".join(\"%.4f\" % -x for x in cv_scores))\n",
    "print(\"Mean CV MSE = %.4f\" % np.mean(-cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYdumLg_P56S"
   },
   "outputs": [],
   "source": [
    "def show_weights(features, weights, scales):\n",
    "    fig, axs = plt.subplots(figsize=(14, 10), ncols=2)\n",
    "    sorted_weights = sorted(zip(weights, features, scales), reverse=True)\n",
    "    weights = [x[0] for x in sorted_weights]\n",
    "    features = [x[1] for x in sorted_weights]\n",
    "    scales = [x[2] for x in sorted_weights]\n",
    "    sns.barplot(y=features, x=weights, ax=axs[0])\n",
    "    axs[0].set_xlabel(\"Weight\")\n",
    "    sns.barplot(y=features, x=scales, ax=axs[1])\n",
    "    axs[1].set_xlabel(\"Scale\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "GV8DIIIsP56S",
    "outputId": "b290b7e4-25bf-4dfb-e72e-8c75790c47a5"
   },
   "outputs": [],
   "source": [
    "show_weights(X_train.columns, model.coef_, X_train.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVeKGlLiP56S"
   },
   "source": [
    "### Feature Transform\n",
    "\n",
    "[`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLmKzNFRP56S",
    "outputId": "552767f8-ec07-4dc4-c461-bb04b45cf3b4"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = Ridge()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "print(\"Test MSE = %.4f\" % mean_squared_error(y_test, y_pred))\n",
    "print(\"Train MSE = %.4f\" % mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "SybQ-A8RP56T",
    "outputId": "7f8c7b52-636d-4cd5-bbc5-3a4d1753021d"
   },
   "outputs": [],
   "source": [
    "scales = pd.Series(data=X_train_scaled.std(axis=0), index=X_train.columns)\n",
    "show_weights(X_train.columns, model.coef_, scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fU7dQ2OQP56T"
   },
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "CcnYG1MrP56T",
    "outputId": "5439149c-47d9-4644-b717-eea7d490ec13"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = np.logspace(-2, 3, 20)\n",
    "searcher = GridSearchCV(Ridge(), [{\"alpha\": alphas}], \n",
    "                        scoring=\"neg_mean_squared_error\", cv=5)\n",
    "searcher.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_alpha = searcher.best_params_[\"alpha\"]\n",
    "print(\"Best alpha = %.4f\" % best_alpha)\n",
    "\n",
    "plt.plot(alphas, -searcher.cv_results_[\"mean_test_score\"])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"CV score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDi8W5j9P56T"
   },
   "source": [
    "**Question**: Why don't we choose the regularization coefficient by train or test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjMQqFYJP56U"
   },
   "source": [
    "### l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "du3DKE29P56U",
    "outputId": "048948c2-f73a-4093-ad01-990b981cfc22"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE = %.4f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj8iUXjVP56U"
   },
   "source": [
    "#### residuals distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80VQH9ExP56V",
    "outputId": "9c0886a8-a014-4793-9a5a-88ea1ff1d4c1"
   },
   "outputs": [],
   "source": [
    "error = (y_train - model.predict(X_train)) ** 2\n",
    "sns.distplot(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_37IOUjpP56V"
   },
   "source": [
    "There are examples with large residuals. Let's drop them from the training set. For instance, drop the examples with residuals greater than 0.95-quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSTsAlUzP56V"
   },
   "outputs": [],
   "source": [
    "mask = (error < np.quantile(error, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKr7Op5SP56V",
    "outputId": "fbc55e33-8d38-4312-c923-e3545dd26895"
   },
   "outputs": [],
   "source": [
    "model = Lasso().fit(X_train[mask], y_train[mask])\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMUZ9GgFP56V"
   },
   "outputs": [],
   "source": [
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7tfhhHWP56V",
    "outputId": "618ca16d-8cdc-4fa8-bdac-dce926848159"
   },
   "outputs": [],
   "source": [
    "error = (y_train[mask] - model.predict(X_train[mask])) ** 2\n",
    "sns.distplot(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "04_HSE_SE_Linear_regression_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
