{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjoPNHSl1tUJ"
   },
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Seminar 8: Ensembles, RandomForest\n",
    "**Author**: Maria Tikhonova\n",
    "\n",
    "<center> \n",
    "    <table>\n",
    "        <tr>\n",
    "            <th> Class Teachers </th>\n",
    "            <th> Contact </th>\n",
    "            <th> Group </th>\n",
    "            <th> TA (contact) </th>\n",
    "        </tr> \n",
    "        <tr><td> Andrey Egorov </td><td> tg: @andrei_egorov </td><td> БПИ201, БПИ202 </td><td> Andrei Dyadynov (tg: @mr_dyadyunov), Nikita Tatarinov (tg: @NickyOL) </td></tr>\n",
    "        <tr><td> Kirill Bykov </td><td> tg: @darkydash </td><td> БПИ203, БПИ204 </td><td> Anastasia Egorova (tg: @wwhatisitt), Elizaveta Berdina (tg: @berdina_elis) </td></tr>\n",
    "        <tr><td> Maria Tikhonova </td><td> tg: @mashkka_t </td><td> БПИ205 </td><td> Alexander Stepin (tg: @kevicia) </td></tr>\n",
    "        <tr><td> Anastasia Voronkova </td><td> tg: @kotovasyka </td><td> БПИ206, БПИ207 </td><td> Anton Alekseev (tg: @flameglamebeatskilla), Emil Akopyan (tg: @archivarius) </td></tr>        \n",
    "    </table>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Npc8L7Pp1tUO"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Increase viewable area of Pandas tables, numpy arrays, plots\n",
    "pd.set_option('max_rows', 15, 'max_columns', 500, 'max_colwidth', 1, 'precision', 2)\n",
    "np.set_printoptions(linewidth=10000, precision=4, edgeitems=20, suppress=True)\n",
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = [16, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPc-rQVl1tUQ"
   },
   "source": [
    "# 1. Bias-Variance Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di-OLf721tUT"
   },
   "source": [
    "Consider regression task with MSE error metric. Suppose the following true relationship: \n",
    "\n",
    "$$ \n",
    "y(x) = f(x) + \\epsilon \n",
    "$$ \n",
    "\n",
    "where $ \\epsilon $ is gaussian white noise, $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2) $. Denote the regression model predictions as $ a(x) = \\hat{y} $. Then, for the error expected value we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Error & = \\mathbb{E}_{X,Y,\\epsilon}\\left[(a(x) - y(x))^2\\right] = \\\\\n",
    "      & = \\mathbb{E}_{X,Y}\\left[(a(x) - \\mathbb{E}_{X,Y}[a(x)])^2\\right]  + \\left(\\mathbb{E}_{X,Y}\\left[a(x)\\right] - f(x)\\right)^2 +  \\sigma^2 = \\\\ & = \\text{Variance} + \\text{Bias}^2  + \\text{Noise}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Those components can be interpreted as\n",
    "* $\\text{Variance}$ (Разброс) - model's sensivity to dataset. High variance usually means that model is overfitted.\n",
    "* $\\text{Bias}$ (Смещение) - is in charge of model's precision. High bias usually means that model is underfitted.\n",
    "* $\\text{Noise}$ (Шум) - that is just noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuemCwVO1tUX"
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvBxnzo41tUY"
   },
   "source": [
    "Consider an example of a regression task with kNN regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOFLSpgR1tUZ"
   },
   "source": [
    "### One model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6eSDRjU1tUZ"
   },
   "outputs": [],
   "source": [
    "# Define target function for regression\n",
    "def f(x):\n",
    "    return np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ck4ri7l1tUa"
   },
   "source": [
    "Generate train and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-q9prS71tUb"
   },
   "outputs": [],
   "source": [
    "# Define noise level\n",
    "sigma = 0.2\n",
    "\n",
    "# Generate train sample\n",
    "X_train = np.random.random(size=5) * 2 * np.pi\n",
    "y_train = f(X_train) + np.random.normal(loc=0., scale=sigma, size=len(X_train))\n",
    "\n",
    "# Generate test smaple\n",
    "X_test = np.linspace(0, 2*np.pi, 1000)\n",
    "y_test = f(X_test) + np.random.normal(loc=0., scale=sigma, size=len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1ufKjCJ1tUd"
   },
   "source": [
    "Fit a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_H7tZGU1tUd",
    "outputId": "e4a12c54-26e6-448e-e274-aa70e60d8f72"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Create a model\n",
    "reg = KNeighborsRegressor(n_neighbors=1)\n",
    "\n",
    "# Fit the model\n",
    "reg.fit(X_train.reshape(-1, 1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ8SFAsK1tUe"
   },
   "source": [
    "Make prediction on test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FznmW8hE1tUe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions\n",
    "y_test_predict = reg.predict(X_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC_9U2yn1tUf"
   },
   "source": [
    "Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "xAY8LuEl1tUf",
    "outputId": "8e27ec4b-a538-48c7-b266-34a54d6a57cc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(X_test, f(X_test), label='f(x)', color='b', linewidth=3)\n",
    "plt.plot(X_test, y_test_predict, label='kNN model', color='r', linewidth=3)\n",
    "plt.scatter(X_test, y_test, label='y_test', color='0.5')\n",
    "\n",
    "plt.xticks(size=14)\n",
    "plt.xlabel(\"X\", size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylabel(\"y\", size=14)\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbNVVwbF1tUf"
   },
   "source": [
    "Plot prediction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOMCx_EB1tUf"
   },
   "outputs": [],
   "source": [
    "errors = (y_test_predict - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "EvefuXnM1tUg",
    "outputId": "3eab939f-2129-487b-e870-0e7ab07ad23e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.scatter(X_test, errors, label='Errors', color='r')\n",
    "\n",
    "plt.xticks(size=14)\n",
    "plt.xlabel(\"X\", size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylabel(\"Error\", size=14)\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SoDgDF21tUh"
   },
   "source": [
    "## Several models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-OCOubA1tUi"
   },
   "source": [
    "Train Several Models on Random Train Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-c9aLiq1tUi"
   },
   "outputs": [],
   "source": [
    "# Define noise level\n",
    "sigma = 0.2\n",
    "\n",
    "# Generate train sample\n",
    "X_train = np.random.random(size=5) * 2 * np.pi\n",
    "y_train = f(X_train) + np.random.normal(loc=0., scale=sigma, size=len(X_train))\n",
    "\n",
    "# Generate test smaple\n",
    "X_test = np.linspace(0, 2*np.pi, 1000)\n",
    "y_test = f(X_test) + np.random.normal(loc=0., scale=sigma, size=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Do6q97U1tUi"
   },
   "outputs": [],
   "source": [
    "N_iter = 1000\n",
    "predictions = []\n",
    "errors = []\n",
    "\n",
    "for i in range(N_iter):\n",
    "    \n",
    "    # Generate random train sample\n",
    "    X_train = np.random.random(size=5) * 2 * np.pi\n",
    "    y_train = f(X_train) + np.random.normal(loc=0., scale=sigma, size=len(X_train))\n",
    "    \n",
    "    # Create a model\n",
    "    reg = KNeighborsRegressor(n_neighbors=1)\n",
    "    \n",
    "    # Fit the model\n",
    "    reg.fit(X_train.reshape(-1, 1), y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_test_predict = reg.predict(X_test.reshape(-1, 1))\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions.append(y_test_predict)\n",
    "        \n",
    "    # Generate random test y\n",
    "    y_test_iter = f(X_test) + np.random.normal(loc=0., scale=sigma, size=len(X_test))\n",
    "\n",
    "    # Calculate errors\n",
    "    errors_iter = (y_test_predict - y_test_iter)**2\n",
    "\n",
    "    # Save errors\n",
    "    errors.append(errors_iter)\n",
    "    \n",
    "predictions = np.array(predictions)\n",
    "errors = np.array(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZSGuqQt1tUi"
   },
   "source": [
    "Estimate expected model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0W6asjiN1tUj"
   },
   "outputs": [],
   "source": [
    "y_test_predict_mean = predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9zwA8xc1tUj"
   },
   "source": [
    "Plot models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "BGB58YcI1tUj",
    "outputId": "e83bbc1b-99cb-45ec-e433-86f510674d6c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "for y_test_predict in predictions[:100]:\n",
    "    plt.plot(X_test, y_test_predict, color='r', linewidth=1, alpha=0.5)\n",
    "\n",
    "plt.plot(X_test, predictions[0], label='Prediction', color='r', linewidth=1, alpha=0.5)\n",
    "plt.plot(X_test, y_test_predict_mean, label='Prediction mean', color='0', linewidth=3)\n",
    "plt.plot(X_test, f(X_test), label='f(x)', color='b', linewidth=3)\n",
    "plt.scatter(X_test, y_test, label='y_test', color='0.5')\n",
    "\n",
    "plt.xticks(size=14)\n",
    "plt.xlabel(\"X\", size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylabel(\"y\", size=14)\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csojHe451tUl"
   },
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssEy-gdC1tUl"
   },
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Error & = \\mathbb{E}_{X,Y,\\epsilon}\\left[(a(x) - y(x))^2\\right] = \\\\\n",
    "      & = \\mathbb{E}_{X,Y}\\left[(a(x) - \\mathbb{E}_{X,Y}[a(x)])^2\\right]  + \\left(\\mathbb{E}_{X,Y}\\left[a(x)\\right] - f(x)\\right)^2 +  \\sigma^2 = \\\\ & = \\text{Variance} + \\text{Bias}^2  + \\text{Noise}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swCBjZ0Z1tUl"
   },
   "outputs": [],
   "source": [
    "# Calculate expected errors\n",
    "error = errors.mean(axis=0)\n",
    "\n",
    "# Calculate varince of the model predictions\n",
    "variance = predictions.std(axis=0)**2\n",
    "\n",
    "# Calculate the model's bias\n",
    "bias2 = (y_test_predict_mean - f(X_test))**2\n",
    "\n",
    "# Calculate the noise variance\n",
    "noise = sigma**2\n",
    "\n",
    "composition = variance + bias2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "1-ZNgCmW1tUl",
    "outputId": "ddca7ef0-dc08-4c0f-96ad-4a7ef5e8ac6e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(X_test, error, label='Error', color='b', linewidth=5)\n",
    "plt.plot(X_test, composition, label='Variance + Bias2 + Noise', color='0', linewidth=3)\n",
    "plt.plot(X_test, variance, label='Variance', color='r', linewidth=3)\n",
    "plt.plot(X_test, bias2, label='Bias2', color='0.4', linewidth=3)\n",
    "plt.plot(X_test, [noise]*len(X_test), label='Noise', color='orange', linewidth=3)\n",
    "\n",
    "plt.xticks(size=14)\n",
    "plt.xlabel(\"X\", size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.ylabel(\"Error\", size=14)\n",
    "plt.legend(fontsize=14, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx86wDoL3OmG"
   },
   "source": [
    "## Bonus part on theory\n",
    "**Attention!** This material is quite difficult! [Here](https://github.com/esokolov/ml-course-hse/blob/master/2020-fall/seminars/sem08-bvd.pdf) is a good explanation (in Russian).\n",
    "\n",
    "For simplicity let us consider the case of one-dimensional linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5iwHgND4-QU"
   },
   "source": [
    "**Training algorithm**\n",
    "\n",
    "For one-dimensional linear regression the relationship between $y$ and $x$ is modeled by the simple linear function $y = kx$. The optimal parameter $k$ is found on training set $X = \\{(x_1, y_1), \\dots (x_\\ell, y_\\ell)\\}$ via minimization of $\\text{MSE}= \\sum_{i=1}^\\ell (y_i - k x_i)^2$. \n",
    "\n",
    "As a result we obtain the following algorithm $\\mu(X)$ :\n",
    "    $$\n",
    "    \\mu(X)(x) = k(X ) x, \\quad k(X) = \\frac{\\sum_i x_i y_i}{\\sum_i x_i^2}.\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDGZX3EA3p6p"
   },
   "source": [
    "**Generation model**\n",
    "\n",
    "Assume that $x$ is generated from normal distribution: \n",
    "$$x \\sim p(x) = \\mathcal{N}(0, \\sigma_1^2).$$\n",
    "\n",
    "The true label of $x$ is defined by some function with noise: \n",
    "$$f(x): y = f(x) + \\varepsilon,$$ \n",
    "$$\\varepsilon \\sim p(\\varepsilon) = \\mathcal{N}(0, \\sigma_2^2.)$$ \n",
    "\n",
    "In other words:\n",
    "$$y \\sim p(y|x) = \\mathcal{N}(f(x), \\sigma_2^2)$$ \n",
    "    \n",
    " A sample $X$ consists of $\\ell$ independent pairs $(x_i, y_i)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FUEm1dU6OAW"
   },
   "source": [
    "**Problem 1.1** Find the noise component for the one-dimensional linear regression.\n",
    "\n",
    "**Solution**  \n",
    "\n",
    "$p(y|x)$ is normal, thus, for it we can simply find the expected value:\n",
    "    $$\n",
    "    \\mathbb{E}[y|x] = f(x).\n",
    "    $$\t\n",
    "\n",
    "  Thus:\n",
    "\n",
    "$$\n",
    "    \\mathbb{E}_{x, y}\\bigl[(y - \\mathbb{E}[y|x] )^2\\bigr] = \n",
    "    \\mathbb{E}_{x, \\varepsilon}\\bigl[(f(x) + \\varepsilon - f(x) )^2\\bigr] = \n",
    "    \\mathbb{E}_{\\varepsilon} \\varepsilon^2 = \\mathbb{D}\\varepsilon + \\bigl(\\mathbb{E}\\varepsilon\\bigr)^2 = \\sigma_2^2 + 0 = \\sigma_2^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dkVLRD71tUV"
   },
   "source": [
    "\n",
    "    \n",
    "\n",
    "    \n",
    "**Problem 1.2**\n",
    "\n",
    "Find the bias component for one-dimensional linear regression $f(x) = ax$.  \n",
    "\n",
    "**Solution:**\n",
    "\n",
    "For a start let us find the \"average on all the samples\" answer of the algorithm on $x$:\n",
    "\n",
    "$$\n",
    "    \t\\mathbb{E}_{X}[\\mu(X)(x)] = \\mathbb{E}_{X}[k(X)] \\, x.\n",
    "$$\n",
    "        \n",
    "We need to find the \"average\" $k$ on all the samples:\n",
    "    \t\n",
    "$$ \\mathbb{E}_{X}[k(X)] = \\int \\frac{\\sum_i x_i (f(x_i)+\\varepsilon_i)}{\\sum_i x_i^2} \\prod_i \\bigl( p(x_i) p(\\varepsilon_i) \\bigr)  dx_1 \\dots dx_\\ell \\, d\\varepsilon_1 \\dots d\\varepsilon_\\ell.\n",
    "$$\n",
    "This is an improper integral, which is not always can be calculated analytically.\n",
    "\n",
    "However, when the dependency is linear we get:\n",
    "    \t$$\n",
    "    \t\\mathbb{E}_{X}[k(X)] =  \\int \\frac{\\sum_i x_i (a\\,x_i+\\varepsilon_i)}{\\sum_i x_i^2}  p(\\bar x) p(\\bar \\varepsilon)  d \\bar x d \\bar \\varepsilon = \n",
    "    \t$$\n",
    "        ...\n",
    "    \t$$\n",
    "    \t=a \\int \\frac{\\sum_i x_i^2}{\\sum_i x_i^2}  p(\\bar x)  p(\\bar \\varepsilon)  d \\bar x d \\bar \\varepsilon +\n",
    "    \t\\int \\frac{\\sum_i x_i \\varepsilon_i}{\\sum_i x_i^2}  p(\\bar x)  p(\\bar \\varepsilon)  d \\bar x d \\bar \\varepsilon.\n",
    "    \t$$\n",
    "The first integral equals $a$ (as the integral over the entire space of the density function). The second integral equals zero as long as integrand (подынтегральная функция) is odd. Thus, the \"average\" coefficient equals  $a$.\n",
    "    \t\n",
    "Let us find the bias component:\n",
    "  $$\n",
    "    \t\\mathbb{E}_{x}\\bigl[(\\mathbb{E}_{X}[\\mu(X)(x)] - \\mathbb{E}[y|x] )^2\\bigr] = \\mathbb{E}_{x}\\bigl[(a x -  a x )^2\\bigr] = 0.\n",
    "  $$\n",
    "\n",
    "This result is quite intuitive: if we iterate over all possible samples of length $\\ell$ and average the value of $k$, then we will be able to find the true value of the coefficient.\n",
    "    \t\n",
    "    \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pepFLAxy-Ogw"
   },
   "source": [
    "**Problem 1.3** Find the variance component for one-dimensional linear regression $f(x) = ax$.  \n",
    "\n",
    "$$\n",
    "    \t\\mathbb{E}_{x}\\bigl[\\mathbb{E}_{X}\\bigl[(\\mu(X)(x) - \\mathbb{E}_{X}[\\mu(X)(x)] )^2\\bigr]\\bigr] = \n",
    "    \t\\mathbb{E}_{x}\\bigl[\\mathbb{E}_{X}\\bigl[(\n",
    "    \ta x +  \\frac{\\sum_i x_i \\varepsilon_i}{\\sum_i x_i^2} x - ax\n",
    "    \t )^2\\bigr]\\bigr]  =\\biggl( \\mathbb{E}_{x} x^2 \\biggr) \n",
    "    \t \\biggl (\\mathbb{E}_{X}\\biggl(\n",
    "    \t \\frac{\\sum_i x_i \\varepsilon_i}{\\sum_i x_i^2} \n",
    "    \t\\biggr )^2\\biggr) = \n",
    "$$\n",
    "      \n",
    "$$=\\sigma_1^2\\mathbb{E}_{X}\\biggl (\\frac{\\sum_i x_i \\varepsilon_i}{\\sum_i x_i^2} \\biggr)^2.\n",
    "$$\n",
    "\n",
    "We can simplify the expected value:\n",
    "\n",
    "$$\n",
    "    \t\\mathbb{E}_{X}\\biggl(\n",
    "    \t\\frac{\\sum_i x_i \\varepsilon_i}{\\sum_i x_i^2} \n",
    "    \t\\biggr )^2 = \\mathbb{E}_{\\bar x} \\biggl[ \\frac {\\sum_{i \\ne j} x_i x_j \\mathbb{E}_{\\bar \\varepsilon } [\\varepsilon_i \\varepsilon_j] +\n",
    "    \t\t\\sum_i x_i^2 \\mathbb{E}_{\\bar \\varepsilon } \\varepsilon_i^2 } {(\\sum_i x_i^2)^2} \n",
    "    \t \\biggr].\n",
    "$$\n",
    "\n",
    "As long as $\\varepsilon_i$ and $\\varepsilon_j$ are independent, $\\mathbb{E}[\\varepsilon_i \\varepsilon_j] = 0$, and $\\mathbb{E}_{\\bar \\varepsilon } \\varepsilon_i^2 = \\sigma_2^2$. \n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "    \t\\mathbb{E}_{X}\\biggl(\n",
    "    \t\\frac{\\sum_i x_i \\varepsilon_i}{\\sum_i x_i^2} \n",
    "    \t\\biggr )^2 = \\mathbb{E}_{\\bar x} \\biggl[ \\frac {\n",
    "    \t\t\\sum_i x_i^2 \\sigma_2^2} {(\\sum_i x_i^2)^2} \n",
    "    \t\\biggr] = \\sigma_2^2 \\mathbb{E}_{\\bar x} \\biggl[ \\frac 1 {\\sum_i x_i^2} \\biggr],\n",
    "$$\n",
    "\n",
    "an variance:\n",
    "$$\n",
    "    \t\\mathbb{E}_{x}\\bigl[\\mathbb{E}_{X}\\bigl[(\\mu(X)(x) - \\mathbb{E}_{X}[\\mu(X)(x)] )^2\\bigr]\\bigr] = \n",
    "    \t \\sigma_1^2 \\sigma_2^2\n",
    "    \t\\mathbb{E}_{\\bar x} \\biggl[ \\frac 1 {\\sum_i x_i^2} \\biggr].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_BMgzsk1tUm"
   },
   "source": [
    "# 2. Ensembles\n",
    "\n",
    "### Data  - Titanic data set\n",
    "\n",
    "Today we are going to work with the Titatic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6mAHtgro1tUm",
    "outputId": "3f53bcf6-456e-46c4-c6ac-56d869bcf449"
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('titanic.csv')\n",
    "data = pd.read_csv('https://grantmlong.com/data/titanic.csv')\n",
    "cols_2_drop = ['PassengerId', 'Ticket', 'Cabin', 'Name']\n",
    "data = data.drop(cols_2_drop, axis=1)\n",
    "data.loc[:, 'Age'] = data.Age.fillna(-999)\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8p6qxN1tUm"
   },
   "source": [
    "## Mean label encoding\n",
    "\n",
    "We have two categorical variables: `Sex` and `Embarked`. \n",
    "\n",
    "Today we will use the new technique for their encoding: **mean label encoding**.\n",
    "\n",
    "The idea of **mean label encoding**: for each unique value of the categorical feature, encode it based **on the ratio of occurrence of the positive class in the target variable**.\n",
    "\n",
    "**Be careful!**\n",
    "\n",
    "The fact that we are encoding the feature based on target classes may lead to data leakage and overfitting. To solve this, mean encoding is usually used with some type of Regularization. \n",
    "\n",
    "More information can be found [here](https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RSDO0rrCHOB"
   },
   "source": [
    "**Mean label encoding** for the Sex variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBnGTk3V1tUm",
    "outputId": "a0344936-4a05-43e7-cb5c-689ab862e9f5"
   },
   "outputs": [],
   "source": [
    "mean_sex = data.groupby('Sex')['Survived'].mean()\n",
    "#the ratio of occurrence of the positive class in the target variable\n",
    "mean_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhlDPV9F1tUn"
   },
   "outputs": [],
   "source": [
    "data['Sex'] = data['Sex'].apply(lambda x: mean_sex[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPMy_kmZCPeb"
   },
   "source": [
    "**Mean label encoding** for the Embarked variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-8wzhbiy1tUn",
    "outputId": "8265cbe0-c72a-4c20-ffe5-3d7234cf98e9"
   },
   "outputs": [],
   "source": [
    "mean_emb = data[['Embarked', 'Survived']].fillna('No').groupby('Embarked')['Survived'].mean()\n",
    "data['Embarked'] = data['Embarked'].fillna('No').apply(lambda x: mean_emb[x])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIDuEyI_1tUn",
    "outputId": "03a3dac9-163f-4658-ed14-884b77468ff3"
   },
   "outputs": [],
   "source": [
    "X = data.drop('Survived', axis=1)\n",
    "Y = data['Survived']\n",
    "#Train test split\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
    "train_X.shape, test_X.shape, train_Y.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06m8VpVQ1tUn"
   },
   "source": [
    "## Recap: high variance of decision trees \n",
    "\n",
    "It's said that Decision trees have **high variance**. Small changes in data can lead to radically different trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQplG-HQ1tUo"
   },
   "outputs": [],
   "source": [
    "def plot_tree(model, X, fname='temp_tree.png'):\n",
    "    dot_data = export_graphviz(model, filled=True, rounded=True, feature_names=X.columns, out_file=None)\n",
    "    pydot_graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    pydot_graph.write_png(fname)\n",
    "    img = plt.imread(fname)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zdACDi_1tUo",
    "outputId": "2599bf33-91df-4d54-b845-e31bd81717ce"
   },
   "outputs": [],
   "source": [
    "# Random state affects the order at which predictors are considered. At each split the predictors are randomly permuted\n",
    "tree1 = DecisionTreeClassifier(random_state=0, max_depth=3) \n",
    "tree1.fit(train_X, train_Y)\n",
    "tree1_r2 = r2_score(tree1.predict(test_X), test_Y)\n",
    "tree1_accuracy = accuracy_score(tree1.predict(test_X), test_Y)\n",
    "print(f'Accuracy for the first tree: {tree1_accuracy:.4f}')\n",
    "train_X_2, test_X_2, train_Y_2, test_Y_2 =train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "\n",
    "tree2 = DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "tree2.fit(train_X_2, train_Y_2)\n",
    "\n",
    "tree2_accuracy = accuracy_score(tree2.predict(test_X_2), test_Y_2)\n",
    "\n",
    "print(f'Accuracy for the second tree: {tree2_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "19cxU0DK1tUp",
    "outputId": "67f1c033-5572-40c6-d615-a8fd6c0bbd12"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_tree(tree1, train_X)\n",
    "plt.axis('off')\n",
    "plt.title(f'Tree 1')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_tree(tree2, train_X_2)\n",
    "plt.axis('off')\n",
    "plt.title(f'Tree 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGasH7tF1tUp"
   },
   "source": [
    "The dataset is the same and only 20% of data is selected into the test set, but trees are very different and achieve different results. \n",
    "\n",
    "It only gets worse if we use deeper trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPjdGFcu1tUq"
   },
   "source": [
    "## Bootstrap aggregated Decision Trees\n",
    "Instead of training just one tree, let's do the following:\n",
    "1. Make $B$ bootstrap (sampling with replacement, remember?) samples from the training set.\n",
    "2. Fit a decision tree (deep) to each, obtaining $B$ trees $f_1, f_2, \\dots, f_B$\n",
    "3. To predict, average the prediction of all trees: $\\hat{y} = 1/B \\sum_{i=0}^{B} f_i(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Su0tzCRW1tUq"
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FFedbB41tUq",
    "outputId": "d481a3ce-2e59-4f75-ac94-4bd9c507058f"
   },
   "outputs": [],
   "source": [
    "# BaggingRegressor can be used with any model, not only decision trees\n",
    "\n",
    "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3), n_jobs=-1, n_estimators=100)\n",
    "model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jZki6nt1tUr",
    "outputId": "943052ad-4f10-4f1e-8e60-2c7d78427d5a"
   },
   "outputs": [],
   "source": [
    "print(f'Bagged trees accuracy score {accuracy_score(model.predict(test_X), test_Y):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyK2DbE_1tUr"
   },
   "source": [
    "We can access individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "5gTy1LYH1tUs",
    "outputId": "d3a7dcb8-ed11-4e6d-8315-e580c0841c23"
   },
   "outputs": [],
   "source": [
    "tree1, tree2 = model.estimators_[0], model.estimators_[1]\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_tree(tree1, train_X)\n",
    "plt.axis('off')\n",
    "plt.title(f'Tree 1 of the bagging ensemble')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_tree(tree2, train_X)\n",
    "plt.axis('off')\n",
    "plt.title(f'Tree 2 of the bagging ensemble')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL6dran91tUt"
   },
   "source": [
    "The training process of a Bagged ensemble allows us to estimate the test error **without using cross-validation**!\n",
    "\n",
    "Each tree is trained on a sample of the training set.\n",
    "\n",
    "For each observation $x_i$ we can make predictions using the trees for which $x_i$ was not in the training set. Then we can average the predictions and use them to compute the error. This will be a robust test error estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Re_lGuJs1tUt",
    "outputId": "ff6a1630-7f19-48fc-9485-401062900fa2"
   },
   "outputs": [],
   "source": [
    "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=None), n_jobs=-1, n_estimators=20, oob_score=True)\n",
    "model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C72Smf-z1tUt",
    "outputId": "2397b5a9-33d4-4077-b157-4b35cf1d4716"
   },
   "outputs": [],
   "source": [
    "print(f'Test score estimate using Out-of-bag score {model.oob_score_:.4f}')\n",
    "print(f'Test score estimate using Cross-Validation {cross_val_score(model, train_X, train_Y, scoring=\"accuracy\").mean():.4f}')\n",
    "print(f'Actual test score: {accuracy_score(model.predict(test_X), test_Y):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT8U6_AU1tUt"
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "Trees in Bagged ensembles tend to be *correlated*. They tend to be very similar. When you aggregate predictions of many similar trees, you don't reduce variance a lot.\n",
    "\n",
    "How can we make trees less correlated?\n",
    "\n",
    "**The Random Forest solution**: every time we consider a split, **sample $m$ features** out of the total $p$, such that $m < p$, and **make the best split using these $m$ features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeArMc1b1tUu",
    "outputId": "ec65ce48-c04c-436b-dbc0-09f548e8f12a"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth = 3, oob_score=True)\n",
    "model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Oxswnmt1tUu",
    "outputId": "267a4fe7-0e40-4f81-ca34-4d7cbbc19d86"
   },
   "outputs": [],
   "source": [
    "print(f'Test score estimate using Out-of-bag score {model.oob_score_:.4f}')\n",
    "print(f'Actual test score: {accuracy_score(model.predict(test_X), test_Y):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "gev4xHbg1tUu",
    "outputId": "b24d89b9-89bb-4569-86e9-c5cd8a9b2de8"
   },
   "outputs": [],
   "source": [
    "tree1, tree2 = model.estimators_[0], model.estimators_[1]\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_tree(tree1, train_X)\n",
    "plt.axis('off')\n",
    "plt.title(f'Tree 1 of the RF ensemble')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_tree(tree2, train_X)\n",
    "plt.axis('off')\n",
    "plt.title(f'Tree 2 of the RF ensemble')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iP8lItMG1tUw",
    "outputId": "bb6ad002-fef7-47fd-9dae-5bb2878d228f"
   },
   "outputs": [],
   "source": [
    "# Now lets try realistic max_depth\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth = None, oob_score=True)\n",
    "model.fit(train_X, train_Y)\n",
    "print(f'Test score estimate using Out-of-bag score {model.oob_score_:.4f}')\n",
    "print(f'Actual test score: {accuracy_score(model.predict(test_X), test_Y):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNL7eHeW1tUw"
   },
   "source": [
    "## Feature importance\n",
    "\n",
    "For each tree, we can compute the information gain caused by splitting on some feature. Then if we average this reductions over all trees, we obtain a measure of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "DGENeGUp1tUw",
    "outputId": "0ab6e7de-f0ea-4585-c1cd-d37f50ea0f39"
   },
   "outputs": [],
   "source": [
    "feature_importances = zip(train_X.columns, model.feature_importances_)\n",
    "feature_importances = sorted(feature_importances, key=lambda x: x[1])\n",
    "feature_importances = pd.DataFrame(feature_importances, columns=['feature', 'importance'])\n",
    "\n",
    "plt.title('Feature importances for Random Forest')\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpQuaevJ1tUw"
   },
   "source": [
    "## Tuning\n",
    "\n",
    "The main things to tune for this model:\n",
    "* `n_estimators` - number of trees,\n",
    "* `max_samples` - number of samples in bootstrapped training set,\n",
    "* `max_features` - number of features considered for a split,\n",
    "* `max_depth` - max depth of trees.\n",
    "Usually it's advised to keep it unlimited, but it can be beneficial to set a `max_depth` when you want to speed up the algorithm on large data or when the dataset contains a lot of noise.\n",
    "\n",
    "\n",
    "More info [here](https://dyakonov.org/2016/11/14/%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D0%B9-%D0%BB%D0%B5%D1%81-random-forest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0igC99S51tUw",
    "outputId": "75b60fa3-176f-4090-aa30-6b52c6d5653f"
   },
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [5,10,50, 100, 500], 'max_features' : [None, 3], 'max_depth':[None, 3, 5, 10]}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "clf = GridSearchCV(model, parameters, scoring = 'accuracy')\n",
    "\n",
    "clf.fit(train_X, train_Y)\n",
    "pred = clf.predict(test_X)\n",
    "\n",
    "print('Best parameters: ', clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHq6BnXx1tUx"
   },
   "source": [
    "Apart from regression and classification RF can also be used for:\n",
    "* Clustering\n",
    "* Anomaly detection\n",
    "* Dimensionality reduction\n",
    "* Feature selection\n",
    "And possibly more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "if6_FKiI1tUx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Seminar_08_ensembles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
